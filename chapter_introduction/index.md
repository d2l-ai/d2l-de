# Einführung
:label:`chap_introduction`

Bis vor kurzem wurde fast jedes Computerprogramm, mit dem wir täglich interagieren, von Softwareentwicklern aus ersten Prinzipien codiert. Angenommen, wir wollten eine Anwendung schreiben, um eine E-Commerce-Plattform zu verwalten. Nachdem wir ein paar Stunden über ein Whiteboard geknauert hatten, um über das Problem nachzudenken, würden wir die breiten Striche einer funktionierenden Lösung finden, die wahrscheinlich so aussehen könnte: (i) Benutzer interagieren mit der Anwendung über eine Schnittstelle, die in einem Webbrowser oder einer mobilen Anwendung ausgeführt wird; (ii) unsere Anwendung interagiert mit einer kommerziellen Datenbank-Engine, um den Status jedes Benutzers zu verfolgen und Aufzeichnungen über historische Transaktionen zu pflegen; und (iii) im Mittelpunkt unserer Anwendung, die *Business-Logik* (Sie könnten sagen, die *Gehirn*) unserer Anwendung schreibt methodisch detailliert die entsprechende Aktion aus, die unsere Programm sollte in jedem denkbaren Umstand zu nehmen.

Um die *Gehirn* unserer Anwendung zu erstellen, müssten wir jeden möglichen Eckfall durchlaufen, den wir erwarten, indem wir entsprechende Regeln entwickeln. Jedes Mal, wenn ein Kunde klickt, um einen Artikel zu seinem Warenkorb hinzuzufügen, fügen wir einen Eintrag zur Warenkorb-Datenbanktabelle hinzu, wobei die Benutzer-ID der angeforderten Produkt-ID zugeordnet wird. Während nur wenige Entwickler es beim ersten Mal völlig richtig machen (es könnte einige Testläufe dauern, um die Knicke zu erarbeiten), zum größten Teil könnten wir ein solches Programm von den ersten Prinzipien schreiben und es selbstbewusst starten, bevor wir jemals einen echten Kunden* sehen. Unsere Fähigkeit, automatisierte Systeme nach ersten Prinzipien zu entwickeln, die funktionierende Produkte und Systeme vorantreiben, oft in neuartigen Situationen, ist eine bemerkenswerte kognitive Leistung. Und wenn Sie in der Lage sind, Lösungen zu entwickeln, die $100\ %$ der Zeit arbeiten,
*Sie sollten maschinelles Lernen nicht verwenden*.

Glücklicherweise biegen sich viele Aufgaben, die wir automatisieren möchten, für die wachsende Gemeinschaft von Machine Learning (ML) -Wissenschaftlern nicht so leicht zum menschlichen Einfallsreichtum. Stellen Sie sich vor, sich mit den intelligentesten Köpfen, die Sie kennen, um das Whiteboard herum zu kauern, aber dieses Mal bekämpfen Sie eines der folgenden Probleme:

* Schreiben Sie ein Programm, das das Wetter von morgen voraussagt geografisch
Informationen, Satellitenbilder und ein nachgestelltes Fenster des vergangenen Wetters.
* Schreiben Sie ein Programm, das eine Frage aufnimmt, ausgedrückt in Freiformtext, und beantwortet es richtig.
* Schreiben Sie ein Programm, das ein Bild gegeben kann alle Personen identifizieren, die es enthält, zeichnen Umrisse um jeden.
* Schreiben Sie ein Programm, das Benutzer mit Produkten präsentiert, die sie wahrscheinlich genießen, aber unwahrscheinlich sind, im natürlichen Verlauf des Surfens, begegnen.

In jedem dieser Fälle sind selbst Elite-Programmierer nicht in der Lage, Lösungen von Grund auf neu zu codieren. Die Gründe dafür können variieren. Manchmal folgt das Programm, nach dem wir suchen, einem Muster, das sich im Laufe der Zeit ändert, und wir brauchen unsere Programme, um sich anzupassen. In anderen Fällen kann die Beziehung (z. B. zwischen Pixeln und abstrakten Kategorien) zu kompliziert sein und Tausende oder Millionen von Berechnungen erfordern, die außerhalb unseres bewussten Verständnisses liegen (auch wenn unsere Augen die Aufgabe mühelos bewältigen). ML ist das Studium leistungsfähiger Techniken, die *aus *Erfahrung* lernen können. Da ein ML-Algorithmus mehr Erfahrung sammelt, typischerweise in Form von Beobachtungsdaten oder Interaktionen mit einer Umgebung, verbessert sich die Leistung. Vergleichen Sie dies mit unserer deterministischen E-Commerce-Plattform, die nach der gleichen Geschäftslogik funktioniert, egal wie viel Erfahrung anfällt, bis die Entwickler selbst *lernen* und entscheiden, dass es an der Zeit ist, die Software zu aktualisieren. In diesem Buch vermitteln wir Ihnen die Grundlagen des maschinellen Lernens und konzentrieren uns insbesondere auf Deep Learning, eine leistungsstarke Reihe von Techniken, die Innovationen in so vielfältigen Bereichen wie Computer Vision, natürliche Sprachverarbeitung, Gesundheitswesen und Genomik vorantreiben.

## Ein motivierendes Beispiel

Bevor wir mit dem Schreiben beginnen konnten, mussten die Autoren dieses Buches, wie ein Großteil der Arbeitskräfte, koffeiniert werden. Wir hüpften ins Auto und begannen zu fahren. Mit einem iPhone rief Alex „Hey Siri“ aus und erwachte das Spracherkennungssystem des Telefons. Dann befahl Mu „Anweisungen zum Blue Bottle Coffee Shop“. Das Telefon zeigte schnell die Transkription seines Befehls an. Es erkannte auch, dass wir nach Wegbeschreibungen gefragt haben, und startete die Maps-Anwendung, um unsere Anfrage zu erfüllen. Nach dem Start hat die Maps-App eine Reihe von Routen identifiziert. Neben jeder Route zeigte das Telefon eine prognostizierte Transportzeit an. Während wir diese Geschichte für pädagogische Bequemlichkeit erstellt haben, zeigt sie, dass unsere täglichen Interaktionen mit einem Smartphone innerhalb von nur wenigen Sekunden mehrere Modelle des maschinellen Lernens in Eingriff bringen können.

Stellen Sie sich vor, Sie schreiben ein Programm, um auf ein Wake-Wort* wie „Alexa“, „Okay, Google“ oder „Siri“ zu reagieren. Versuchen Sie es selbst in einem Raum mit nichts als einem Computer und einem Code-Editor zu codieren, wie in :numref:`fig_wake_word` dargestellt. Wie würdest du ein solches Programm aus ersten Prinzipien schreiben? Denken Sie darüber nach... das Problem ist schwer. Jede Sekunde sammelt das Mikrofon ungefähr 44.000 Proben. Jede Probe ist eine Messung der Amplitude der Schallwelle. Welche Regel könnte zuverlässig von einem Rohaudio-Snippet zuversichtlich Vorhersagen ``{yes, no}`` abbilden, ob das Snippet das Aktivierungswort enthält? Wenn du feststeckst, mach dir keine Sorgen. Wir wissen auch nicht, wie man ein solches Programm von Grund auf neu schreibt. Deshalb verwenden wir ML.

![Identify an awake word.](../img/wake-word.svg)
:label:`fig_wake_word`

Hier ist der Trick. Oft, selbst wenn wir nicht wissen, wie man einem Computer explizit sagt, wie man von Eingängen zu Ausgängen abbilden soll, sind wir dennoch in der Lage, die kognitive Leistung selbst durchzuführen. Mit anderen Worten, auch wenn Sie es nicht wissen
*wie man einen Computer* programmiert, um das Wort „Alexa“ zu erkennen,
Sie selbst *sind in der Lage, das Wort „Alexa“ zu erkennen. Bewaffnet mit dieser Fähigkeit können wir einen riesigen *Dataset* sammeln, der Beispiele für Audio enthält und diejenigen kennzeichnen, die *do* und die *nicht* das Aktivierungswort enthalten. Im ML-Ansatz versuchen wir nicht, ein System zu entwerfen
*explizit*, um Wake-Wörter zu erkennen.
Stattdessen definieren wir ein flexibles Programm, dessen Verhalten durch eine Reihe von *Parameter* bestimmt wird. Dann verwenden wir den Datensatz, um den bestmöglichen Satz von Parametern zu bestimmen, diejenigen, die die Leistung unseres Programms in Bezug auf ein Maß der Leistung auf der Aufgabe von Interesse verbessern.

Sie können sich die Parameter als Knöpfe vorstellen, die wir drehen können und das Verhalten des Programms manipulieren. Wenn wir die Parameter fixieren, nennen wir das Programm ein *Modell*. Die Menge aller verschiedenen Programme (Input-Output-Mappings), die wir nur durch Manipulation der Parameter erzeugen können, wird eine *Familie* von Modellen genannt. Und das *Meta-Programm*, das unseren Datensatz verwendet, um die Parameter zu wählen, wird als *Learning Algorithm* bezeichnet.

Bevor wir den Lernalgorithmus nutzen können, müssen wir das Problem genau definieren, die genaue Art der Eingänge und Ausgänge festsetzen und eine geeignete Modellfamilie auswählen. In diesem Fall erhält unser Modell ein Audioausschnitt als *Input*, und es erzeugt eine Auswahl unter ``{yes, no}`` als *Output*. Wenn alles nach Plan läuft, sind die Vermutungen des Modells in der Regel korrekt, ob das Snippet das Aktivierungswort enthält (oder nicht).

Wenn wir die richtige Familie von Modellen wählen, dann sollte es eine Einstellung der Knöpfe geben, so dass das Modell jedes Mal, wenn es das Wort „Alexa“ hört, ``yes`` feuert. Da die genaue Wahl des Wake-Wortes willkürlich ist, benötigen wir wahrscheinlich eine Modellfamilie, die ausreichend reich ist, dass es über eine andere Einstellung der Knöpfe nur dann ``yes`` feuern könnte, wenn das Wort „Apricot“ gehört wurde. Wir erwarten, dass die gleiche Modellfamilie für *"Alexa“ -Erkennung* und *"Apricot“ -Erkennung* geeignet sein sollte, weil sie intuitiv ähnliche Aufgaben zu sein scheinen. Allerdings brauchen wir vielleicht eine andere Familie von Modellen ganz, wenn wir mit grundlegend unterschiedlichen Eingängen oder Ausgängen umgehen wollen, sagen wir, ob wir von Bildern zu Bildunterschriften oder von englischen Sätzen zu chinesischen Sätzen abbilden wollen.

Wie Sie vielleicht vermuten, wenn wir nur alle Knöpfe zufällig einstellen, ist es nicht wahrscheinlich, dass unser Modell „Alexa“, „Apricot“ oder ein anderes englisches Wort erkennt. Im Deep Learning ist das *Learning* der Prozess, mit dem wir die richtige Einstellung der Regler entdecken, die das gewünschte Verhalten von unserem Modell erzwingen.

Wie in :numref:`fig_ml_loop` gezeigt, sieht der Trainingsprozess normalerweise so aus:

1. Beginnen Sie mit einem zufällig initialisierten Modell, das nichts Nützliches tun kann.
1. Schnappen Sie sich einige Ihrer beschrifteten Daten (z. B. Audio-Snippets und entsprechende ``{yes, no}`` Etiketten)
1. Picken Sie die Knöpfe so, dass das Modell in Bezug auf diese Beispiele weniger saugt
1. Wiederholen Sie diesen Vorgang, bis das Modell genial ist.

![A typical training process. ](../img/ml-loop.svg)
:label:`fig_ml_loop`

Um zusammenzufassen, anstatt eine Reake-Worterkennung zu codieren, kodieren wir ein Programm, das *lernen* kann, um Wake-Worte,
*, wenn wir es mit einem großen beschrifteten Datensatz präsentieren*.
Sie können sich diesen Akt vorstellen, das Verhalten eines Programms zu bestimmen, indem Sie es mit einem Datensatz als *Programmierung mit Daten* präsentieren. Wir können einen Katzendetektor „programmieren“, indem wir unser maschinelles Lernsystem mit vielen Beispielen von Katzen und Hunden versorgen, wie die folgenden Bilder:

|cat|cat|dog|dog|
|:---------------:|:---------------:|:---------------:|:---------------:|
|![cat3](../img/cat3.jpg)|![](../img/cat2.jpg)|![](../img/dog1.jpg)|![](../img/dog2.jpg)|

Auf diese Weise wird der Detektor schließlich lernen, eine sehr große positive Zahl zu emittieren, wenn es eine Katze ist, eine sehr große negative Zahl, wenn es ein Hund ist, und etwas näher an Null, wenn es nicht sicher ist, und das kratzt kaum die Oberfläche dessen, was ML tun kann.

Deep Learning ist nur eine von vielen beliebten Methoden zur Lösung von Problemen mit maschinellem Lernen. Bisher haben wir nur über maschinelles Lernen im Großen und nicht über Deep Learning gesprochen. Um zu sehen, warum Deep Learning wichtig ist, sollten wir einen Moment Pause machen, um einige entscheidende Punkte hervorzuheben.

Erstens sind die Probleme, die wir so weit diskutiert haben — das Lernen aus dem rohen Audiosignal, den rohen Pixelwerten von Bildern oder das Mapping zwischen Sätzen beliebiger Länge und ihren Gegenstücken in Fremdsprachen — Probleme, bei denen Deep Learning sich auszeichnet und bei denen traditionelle ML-Methoden ins Stocken geraten. Tiefe Modelle sind *tief* in genau dem Sinne, dass sie viele *Ebenen* der Berechnung lernen. Es stellt sich heraus, dass diese vielschichtigen (oder hierarchischen) Modelle in der Lage sind, Wahrnehmungsdaten auf niedriger Ebene so zu adressieren, dass frühere Werkzeuge nicht konnten. In vergangenen Tagen bestand der entscheidende Teil der Anwendung von ML auf diese Probleme darin, manuell entwickelte Wege zu finden, die Daten in eine Form umzuwandeln, die für *flach* Modelle zugänglich ist. Ein wesentlicher Vorteil von Deep Learning ist, dass es nicht nur die *flach* Modelle am Ende traditioneller Lern-Pipelines ersetzt, sondern auch den arbeitsintensiven Prozess des Feature-Engineerings. Zweitens hat Deep Learning durch den Austausch eines Großteil der domänenspezifischen Vorprozessing* viele der Grenzen eliminiert, die zuvor durch Computersicht, Spracherkennung, Verarbeitung natürlicher Sprache, medizinische Informatik und andere Anwendungsbereiche getrennt wurden. Probleme.

## Die wichtigsten Komponenten: Daten, Modelle und Algorithmen

In unserem Beispiel „Wake-Wort*“ haben wir einen Datensatz beschrieben, der aus Audio-Snippets und binären Labels besteht, und wir gaben einen handgewellten Eindruck dafür, wie wir ein Modell trainieren* könnten, um eine Zuordnung von Snippets zu Klassifikationen zu annähern. Diese Art von Problem, bei dem wir versuchen, ein bestimmtes unbekanntes *Label* vorherzusagen bekannt *Inputs*, gegeben ein Datensatz bestehend aus Beispielen, für die die Labels bekannt sind, heißt *überwachtes Lernen*, und es ist nur eine von vielen *Arten* von Machine-Learning-Problemen. Im nächsten Abschnitt werden wir einen tiefen Einblick in die verschiedenen ML-Probleme nehmen. Zuerst möchten wir mehr Licht auf einige Kernkomponenten werfen, die uns folgen werden, egal welche Art von ML-Problem wir annehmen:

1. Die *Daten*, aus denen wir lernen können.
2. Ein *Modell* zur Transformation der Daten.
3. Eine *Verlust*-Funktion, die die *Schlechtkeit* unseres Modells quantifiziert.
4. Ein *Algorithmus*, um die Parameter des Modells anzupassen, um den Verlust zu minimieren.

### Daten

Es mag selbstverständlich sein, dass man Data Science ohne Daten nicht machen kann. Wir könnten Hunderte von Seiten verlieren, wenn wir darüber nachdenken, was genau Daten ausmacht, aber vorerst werden wir auf der praktischen Seite irren und uns auf die wichtigsten Eigenschaften konzentrieren, mit denen wir uns beschäftigen müssen. Im Allgemeinen beschäftigen wir uns mit einer Sammlung von *Beispielen* (auch als *Datenpunkte*, *Samples* oder *Instanzen* bezeichnet). Um mit Daten sinnvoll arbeiten zu können, müssen wir in der Regel eine geeignete numerische Darstellung erstellen. Jedes *Beispiel* besteht in der Regel aus einer Sammlung numerischer Attribute, die *Features* genannt werden. In den oben genannten überwachten Lernproblemen wird eine Besonderheit als Vorhersage *target* bezeichnet (manchmal auch als *label* oder *abhängige Variable* bezeichnet). Die gegebenen Features, aus denen das Modell seine Vorhersagen machen muss, können dann einfach als die *Features* bezeichnet werden (oder oft die *Inputs*, *Kovariaten* oder *unabhängige Variablen*).

Wenn wir mit Bilddaten arbeiten, könnte jedes einzelne Foto ein *Beispiel* darstellen, jedes wird durch eine geordnete Liste von numerischen Werten dargestellt, die der Helligkeit jedes Pixels entsprechen. Ein $200\times 200$ Farbfoto besteht aus $200\times200\times3=120000$ numerischen Werten, die der Helligkeit der roten, grünen und blauen Kanäle für jede räumliche Position entsprechen. In einer traditionelleren Aufgabe könnten wir versuchen, vorherzusagen, ob ein Patient überleben wird oder nicht, da ein Standardsatz von Merkmalen wie Alter, Vitalzeichen, Diagnosen usw.

Wenn jedes Beispiel durch die gleiche Anzahl numerischer Werte gekennzeichnet ist, sagen wir, dass die Daten aus Vektoren mit fester Länge* bestehen und wir beschreiben die (konstante) Länge der Vektoren als *Dimensionalität* der Daten. Wie Sie sich vorstellen können, kann feste Länge eine bequeme Eigenschaft sein. Wenn wir ein Modell trainieren wollten, um Krebs in Mikroskopiebildern zu erkennen, bedeuten Eingaben mit fester Länge, dass wir uns um eine Sache weniger Sorgen machen müssen.

Allerdings können nicht alle Daten leicht als Vektoren mit fester Länge dargestellt werden. Obwohl wir vielleicht erwarten, dass Mikroskopbilder von Standardausrüstung stammen, können wir nicht erwarten, dass Bilder, die aus dem Internet abgebaut werden, alle mit der gleichen Auflösung oder Form erscheinen. Für Bilder könnten wir in Erwägung ziehen, sie alle auf eine Standardgröße zuschneiden, aber diese Strategie bringt uns nur so weit. Wir riskieren, Informationen in den ausgeschnittenen Teilen zu verlieren. Darüber hinaus widerstehen Textdaten Darstellungen fester Länge noch hartnäckiger. Betrachten Sie die Kundenrezensionen auf E-Commerce-Websites wie Amazon, IMDB oder TripAdvisor. Einige sind kurz: „Es stinkt!“. Andere wandern für Seiten. Ein wesentlicher Vorteil von Deep Learning gegenüber herkömmlichen Methoden ist die komparative Gnade, mit der moderne Modelle Daten mit variierender Länge* verarbeiten können.

Je mehr Daten wir haben, desto einfacher wird unsere Arbeit. Wenn wir mehr Daten haben, können wir leistungsfähigere Modelle trainieren und weniger stark auf vorgefasste Annahmen zurückgreifen. Der Regimewechsel von (vergleichsweise) kleinen auf Big Data trägt maßgeblich zum Erfolg des modernen Deep Learning bei. Um den Point Home zu steuern, funktionieren viele der aufregendsten Modelle im Deep Learning nicht ohne große Datensätze. Einige andere arbeiten im Low-Data-Regime, sind aber nicht besser als herkömmliche Ansätze.

Schließlich reicht es nicht aus, viele Daten zu haben und geschickt zu verarbeiten. Wir benötigen die *rechte* Daten. Wenn die Daten voll von Fehlern sind oder wenn die ausgewählten Features die Zielmenge des Interesses nicht vorhersagen, wird das Lernen fehlschlagen. Die Situation wird durch das Klischee gut erfasst: *Müll in, Müll*. Darüber hinaus ist eine schlechte prädiktive Performance nicht die einzige mögliche Konsequenz. In sensiblen Anwendungen des maschinellen Lernens, wie Predictive Policing, Resumé Screening und Risikomodellen, die für die Kreditvergabe verwendet werden, müssen wir vor allem auf die Folgen von Mülldaten achten. Ein häufiger Fehlermodus tritt in Datasets auf, in denen einige Personengruppen in den Trainingsdaten nicht dargestellt sind. Stellen Sie sich vor, ein Hautkrebserkennungssystem in freier Wildbahn anzuwenden, das noch nie schwarze Haut gesehen hatte. Fehler können auch auftreten, wenn die Daten nicht nur einige Gruppen unterrepräsentiert, sondern gesellschaftliche Vorurteile widerspiegeln. Wenn beispielsweise frühere Einstellungsentscheidungen verwendet werden, um ein Vorhersagemodell zu trainieren, das zum Screen von Lebensläufen verwendet wird, können Modelle des maschinellen Lernens versehentlich historische Ungerechtigkeiten erfassen und automatisieren. Beachten Sie, dass dies alles passieren kann, ohne dass der Datenwissenschaftler sich aktiv verschworen oder gar bewusst ist.

### Modelle

Die meisten maschinellen Lernens beinhaltet die Transformation* der Daten in gewissem Sinne. Vielleicht möchten wir ein System aufbauen, das Fotos aufnimmt und *Smiley-ness* vorhersagt. Alternativ können wir eine Reihe von Sensorwerten aufnehmen und vorhersagen, wie *normal* vs *anomal* die Messwerte sind. Mit *Modell* bezeichnen wir die Berechnungsmaschine für die Erfassung von Daten eines Typs und das Ausspucken von Vorhersagen eines möglicherweise anderen Typs. Insbesondere interessieren wir uns für statistische Modelle, die anhand von Daten abgeschätzt werden können. Während einfache Modelle durchaus in der Lage sind, angemessen einfache Probleme zu lösen, dehnen die Probleme, auf die wir uns in diesem Buch konzentrieren, die Grenzen klassischer Methoden. Deep Learning unterscheidet sich von klassischen Ansätzen hauptsächlich durch die Reihe leistungsfähiger Modelle, auf die es sich konzentriert. Diese Modelle bestehen aus vielen aufeinanderfolgenden Transformationen der Daten, die von oben nach unten angekettet werden, also dem Namen *Deep Learning*. Auf dem Weg zur Diskussion tiefer neuronaler Netzwerke werden wir einige traditionellere Methoden diskutieren.

###  Objektive Funktionen

Früher haben wir maschinelles Lernen als „Lernen aus Erfahrung“ eingeführt. Indem wir hier lernen*, meinen wir *verbessern* bei irgendeiner Aufgabe im Laufe der Zeit. Aber wer soll sagen, was eine Verbesserung ausmacht? Sie könnten sich vorstellen, dass wir vorschlagen könnten, unser Modell zu aktualisieren, und einige Leute könnten nicht einverstanden sein, ob das vorgeschlagene Update eine Verbesserung oder einen Rückgang darstellt.

Um ein formales mathematisches System von Lernmaschinen zu entwickeln, müssen wir formale Maßnahmen dafür haben, wie gut (oder schlecht) unsere Modelle sind. Im maschinellen Lernen und in der Optimierung allgemein nennen wir diese objektiven Funktionen. Nach Konvention definieren wir in der Regel objektive Funktionen, so dass *unter* *besser* ist. Das ist nur eine Konvention. Sie können jede Funktion $f$ nehmen, für die höher ist besser, und sie in eine neue Funktion $f'$ verwandeln, die qualitativ identisch ist, aber für die niedriger ist besser, indem Sie $f' = -f$ einstellen. Da niedriger besser ist, werden diese Funktionen manchmal als
*Verlustfunktionen* oder *Kostenfunktionen*.

Beim Versuch, numerische Werte vorherzusagen, ist die häufigste Objektivfunktion der quadrierte Fehler $(y-\hat{y})^2$. Bei der Klassifizierung besteht das häufigste Ziel darin, die Fehlerrate zu minimieren, d. h. den Bruchteil der Fälle, in denen unsere Vorhersagen nicht mit der Grundwahrheit übereinstimmen. Einige Ziele (wie quadrierte Fehler) sind einfach zu optimieren. Andere (wie Fehlerrate) sind aufgrund der Nichtdifferenzierbarkeit oder anderer Komplikationen nur schwer zu optimieren. In diesen Fällen ist es üblich, ein *Surrogatobjektiv* zu optimieren.

Typischerweise wird die Verlustfunktion in Bezug auf die Parameter des Modells definiert und hängt vom Dataset ab. Die besten Werte der Parameter unseres Modells werden erlernt, indem der Verlust eines *Trainingsset* minimiert wird, der aus einer Reihe von *Beispielen* besteht, die für das Training gesammelt wurden. Das gute Handeln mit den Trainingsdaten garantiert jedoch nicht, dass wir mit (unsichtbaren) Testdaten gut tun werden. Daher werden wir in der Regel die verfügbaren Daten in zwei Partitionen aufteilen wollen: die Trainingsdaten (für die Anpassung von Modellparametern) und die Testdaten (die zur Auswertung durchgeführt werden), wobei die folgenden zwei Mengen gemeldet werden:

 * **Schulungsfehler: **
 Der Fehler bei den Daten, für die das Modell trainiert wurde. Sie könnten sich vorstellen, dass dies wie die Partituren eines Schülers auf Praxisprüfungen verwendet werden, um sich auf eine echte Prüfung vorzubereiten. Auch wenn die Ergebnisse ermutigend sind, garantiert das keinen Erfolg bei der Abschlussprüfung.
 * **Testfehler: ** Dies ist der Fehler, der bei einem nicht sichtbaren Testsatz aufgetreten ist.
 Dies kann deutlich vom Trainingsfehler abweichen. Wenn ein Modell mit den Trainingsdaten gut funktioniert, aber nicht zu unsichtbaren Daten verallgemeinert, sagen wir, dass es *übermäßig anpassung* ist. In realen Begriffen, ist dies wie die echte Prüfung zu überlegen, obwohl es gut auf Praxistexamen geht.

### Optimierungsalgorithmen

Sobald wir eine Datenquelle und Darstellung, ein Modell und eine gut definierte objektive Funktion haben, benötigen wir einen Algorithmus, der nach den bestmöglichen Parametern für die Minimierung der Verlustfunktion suchen kann. Die beliebtesten Optimierungsalgorithmen für neuronale Netzwerke folgen einem Ansatz, der Gradientenabstieg genannt wird. Kurz gesagt, bei jedem Schritt überprüfen sie, um zu sehen, für jeden Parameter, welche Art und Weise der Trainingssatz Verlust würde sich bewegen, wenn Sie diesen Parameter nur eine kleine Menge. Anschließend aktualisieren sie den Parameter in die Richtung, die den Verlust reduziert.

## Arten von maschinellem Lernen

In den folgenden Abschnitten werden einige *Arten* von Problemen des maschinellen Lernens ausführlicher diskutiert. Wir beginnen mit einer Liste von *Zielen*, d.h. einer Liste von Dingen, die wir gerne maschinelles Lernen tun möchten. Beachten Sie, dass die Ziele durch eine Reihe von Techniken von *wie* ergänzt werden, um sie zu erreichen, einschließlich Arten von Daten, Modellen, Trainingstechniken usw. Die Liste unten ist nur eine Auswahl der Probleme, die ML anpacken kann, um den Leser zu motivieren und uns eine gemeinsame Sprache zur Verfügung zu stellen, wenn wir über mehr sprechen Probleme im gesamten Buch.

### Beaufsichtigtes Lernen

Beaufsichtigtes Lernen befasst sich mit der Aufgabe, *Targets* bei *Inputs* vorherzusagen. Die Ziele, die wir oft *Labels* nennen, werden im Allgemeinen mit *y* bezeichnet. Die Eingabedaten, auch *Features* oder Kovariaten genannt, werden typischerweise $\mathbf{x}$ bezeichnet. Jedes (Eingabe-, Ziel-) Paar wird als *Beispiel* oder *Instanz* bezeichnet. Manchmal, wenn der Kontext klar ist, können wir den Begriff Beispiele verwenden, um auf eine Sammlung von Eingaben zu verweisen, auch wenn die entsprechenden Ziele unbekannt sind. Wir bezeichnen jede bestimmte Instanz mit einem tiefgestellten Index, typischerweise $i$, zum Beispiel ($\mathbf{x}_i, y_i$). Ein Dataset ist eine Sammlung von $n$-Instanzen $\{\mathbf{x}_i, y_i\}_{i=1}^n$. Unser Ziel ist es, ein Modell $f_\theta$ zu produzieren, das jede Eingabe $\mathbf{x}_i$ zu einer Vorhersage $f_{\theta}(\mathbf{x}_i)$ abbildet.

Um diese Beschreibung in einem konkreten Beispiel zu verankern, sollten wir voraussagen, ob ein Patient einen Herzinfarkt haben würde oder nicht. Diese Beobachtung, *Herzinfarkte* oder *kein Herzinfarkt*, wäre unser Label $y$. Die Eingangsdaten $\mathbf{x}$ können Vitalzeichen wie Herzfrequenz, diastolischer und systolischer Blutdruck usw. sein.

Die Überwachung kommt ins Spiel, denn bei der Auswahl der Parameter $\theta$ stellen wir (die Supervisoren) dem Modell einen Datensatz zur Verfügung, der aus *beschrifteten Beispielen* besteht ($\mathbf{x}_i, y_i$) besteht, wobei jedes Beispiel $\mathbf{x}_i$ mit dem richtigen Etikett übereinstimmt.

In probabilistischer Hinsicht sind wir in der Regel daran interessiert, die bedingte Wahrscheinlichkeit $P(y|x)$ zu schätzen. Obwohl es nur eines von mehreren Paradigmen im maschinellen Lernen ist, berücksichtigt überwachtes Lernen die Mehrheit der erfolgreichen Anwendungen des maschinellen Lernens in der Industrie. Teilweise liegt das daran, dass viele wichtige Aufgaben scharf beschrieben werden können, um die Wahrscheinlichkeit von etwas Unbekanntem angesichts eines bestimmten Satzes verfügbarer Daten zu schätzen:

* Prognostizieren Krebs vs nicht Krebs, wenn ein CT-Bild gegeben.
* Prognostizieren Sie die korrekte Übersetzung in Französisch, gegeben einen Satz in Englisch.
* Prognostizieren Sie den Kurs einer Aktie im nächsten Monat basierend auf den Finanzberichtungsdaten dieses Monats.

Selbst mit der einfachen Beschreibung „Ziele aus Eingaben vorhersagen“ kann überwachtes Lernen sehr viele Formen annehmen und erfordern sehr viele Modellierungsentscheidungen, abhängig von Typ, Größe und Anzahl der Eingänge und Ausgänge. Zum Beispiel verwenden wir verschiedene Modelle, um Sequenzen (wie Strings von Texten oder Zeitreihendaten) zu verarbeiten und Vektordarstellungen mit fester Länge zu verarbeiten. Wir werden viele dieser Probleme in den ersten 9 Teilen dieses Buches eingehend besuchen.

Informell sieht der Lernprozess etwa so aus: Schnappen Sie sich eine große Sammlung von Beispielen, für die die Kovariaten bekannt sind, und wählen Sie aus ihnen eine zufällige Teilmenge aus, wobei Sie die Grundwahrheitsbeschriftungen für jeden erwerben. Manchmal sind diese Etiketten möglicherweise bereits erfasste Daten (z. B. starb ein Patient innerhalb des folgenden Jahres?) und andere Male müssen wir möglicherweise menschliche Annotatoren verwenden, um die Daten zu beschriften (zB Bilder zu Kategorien zuweisen).

Zusammen bilden diese Eingänge und die entsprechenden Beschriftungen das Trainingsset. Wir füttern den Trainingsdatensatz in einen überwachten Lernalgorithmus ein, eine Funktion, die als Eingabe einen Datensatz übernimmt und eine andere Funktion ausgibt, *das gelernte Modell*. Schließlich können wir bisher unsichtbare Eingaben an das gelernte Modell einfließen, wobei seine Ausgaben als Vorhersagen des entsprechenden Labels verwendet werden. Der vollständige Prozess wird in :numref:`fig_supervised_learning` gezogen.

![Supervised learning.](../img/supervised-learning.svg)
:label:`fig_supervised_learning`

#### Rückschritt

Vielleicht ist die einfachste beaufsichtigte Lernaufgabe, um Ihren Kopf zu wickeln, *Regression*. Betrachten Sie zum Beispiel eine Reihe von Daten, die aus einer Datenbank von Hausverkäufen gesammelt wurden. Wir könnten eine Tabelle bauen, in der jede Reihe einem anderen Haus entspricht, und jede Spalte entspricht einem relevanten Attribut, wie etwa der Quadratmeterzahl eines Hauses, der Anzahl der Schlafzimmer, der Anzahl der Badezimmer und der Anzahl der Minuten (zu Fuß) zum Stadtzentrum. In diesem Dataset wäre jedes *Beispiel* ein bestimmtes Haus, und der entsprechende *Feature-Vektor* wäre eine Zeile in der Tabelle.

Wenn Sie in New York oder San Francisco leben und nicht der CEO von Amazon, Google, Microsoft oder Facebook sind, könnte der Feature-Vektor (qm Footage, Anzahl Schlafzimmer, Anzahl Badezimmer, zu Fuß erreichbar) für Ihr Zuhause etwa aussehen: $[100, 0, .5, 60]$. Wenn Sie jedoch in Pittsburgh leben, könnte es eher wie $[3000, 4, 3, 10]$ aussehen. Feature-Vektoren wie diese sind für die meisten klassischen Algorithmen des maschinellen Lernens unerlässlich. Wir werden weiterhin den Feature-Vektor entsprechend jedem Beispiel $i$ als $\mathbf{x}_i$ bezeichnen und wir können kompakt auf die vollständige Tabelle verweisen, die alle Feature-Vektoren enthält, als $X$.

Was ein Problem zu einer *Regression* macht, sind eigentlich die Ausgänge. Sagen Sie, dass Sie auf dem Markt für ein neues Zuhause sind. Vielleicht möchten Sie den fairen Marktwert eines Hauses abschätzen, wenn Sie einige Merkmale wie diese haben. Der Zielwert, der Verkaufspreis, ist eine *reale Zahl*. Wenn Sie sich an die formale Definition der Reals erinnern, können Sie jetzt Ihren Kopf kratzen. Häuser verkaufen wahrscheinlich nie für Bruchteile von einem Cent, geschweige denn Preise ausgedrückt als irrationale Zahlen. In Fällen wie diesem, wenn das Ziel tatsächlich diskret ist, aber wo die Rundung auf einer ausreichend feinen Skala stattfindet, werden wir Sprache nur ein wenig missbrauchen und unsere Outputs und Ziele weiterhin als reale Zahlen beschreiben.

Wir bezeichnen jedes einzelne Ziel $y_i$ (entsprechend Beispiel $\mathbf{x}_i$) und die Menge aller Ziele $\mathbf{y}$ (entsprechend allen Beispielen $X$). Wenn unsere Ziele willkürliche Werte in irgendeinem Bereich annehmen, nennen wir dies ein Regressionsproblem. Unser Ziel ist es, ein Modell zu erstellen, dessen Vorhersagen die tatsächlichen Zielwerte genau annähern. Wir bezeichnen das prognostizierte Ziel für jede Instanz $\hat{y}_i$. Mach dir keine Sorgen, wenn die Notation dich betrübt. Wir werden es in den nachfolgenden Kapiteln ausführlicher entpacken.

Viele praktische Probleme sind gut beschriebene Regressionsprobleme. Die Vorhersage der Bewertung, die ein Benutzer einem Film zuweisen wird, kann als Regressionsproblem angesehen werden. Wenn Sie 2009 einen großartigen Algorithmus entwickelt haben, um dieses Kunststück zu erreichen, haben Sie vielleicht den [1-million-dollar Netflix prize](https://en.wikipedia.org/wiki/Netflix_Prize) gewonnen. Die Vorhersage der Aufenthaltsdauer für Patienten im Krankenhaus ist auch ein Regressionsproblem. Eine gute Faustregel ist, dass irgend*Wie viel?* oder *Wie viele?* Problem sollte eine Regression vorschlagen.

* „Wie viele Stunden dauert diese Operation?“: *Regression*
* „Wie viele Hunde sind auf diesem Foto?“: *Regression*.

Wenn Sie Ihr Problem jedoch leicht als „Ist das ein _?“ darstellen können, dann ist es wahrscheinlich, Klassifizierung, eine andere Art von überwachtem Problem, das wir als nächstes behandeln werden. Selbst wenn Sie noch nie mit maschinellem Lernen gearbeitet haben, haben Sie wahrscheinlich informell ein Regressionsproblem durchgearbeitet. Stellen Sie sich zum Beispiel vor, Sie hätten Ihre Abflüsse repariert und Ihr Auftragnehmer $x_1=3$ Stunden damit verbracht, Gunk aus Ihren Abwasserrohren zu entfernen. Dann schickte sie Ihnen eine Rechnung von $y_1 = \$350$. Stellen Sie sich vor, dass Ihr Freund den gleichen Auftragnehmer für $x_2 = 2$ Stunden eingestellt hat und dass sie eine Rechnung von $y_2 = \$250$ erhalten hat. Wenn Sie dann jemand gefragt, wie viel Sie auf der bevorstehenden Gunk-Entfernungsrechnung erwarten, könnten Sie einige vernünftige Annahmen treffen, z. B. mehr Arbeitsstunden kosten mehr Dollar. Sie können auch davon ausgehen, dass es eine Grundgebühr gibt und dass der Auftragnehmer dann pro Stunde berechnet. Wenn diese Annahmen zutreffen, könnten Sie angesichts dieser beiden Datenpunkte bereits die Preisstruktur des Auftragnehmers ermitteln:\ $100 per hour plus \$50, um bei Ihnen zu Hause zu erscheinen. Wenn Sie so viel gefolgt sind, verstehen Sie bereits die Idee auf hoher Ebene hinter der linearen Regression (und Sie haben nur implizit ein lineares Modell mit einem Bias-Term entworfen).

In diesem Fall könnten wir die Parameter herstellen, die exakt den Preisen des Auftragnehmers entsprechen. Manchmal ist das nicht möglich, z.B. wenn ein Teil der Varianz neben Ihren beiden Merkmalen einige Faktoren verdankt. In diesen Fällen werden wir versuchen, Modelle zu lernen, die den Abstand zwischen unseren Vorhersagen und den beobachteten Werten minimieren. In den meisten unserer Kapitel werden wir uns auf einen von zwei sehr häufigen Verlusten konzentrieren, den L1-Verlust, bei dem

$$l(y, y') = \sum_i |y_i-y_i'|$$

und die kleinste mittlere Quadrate Verlust, oder $L_2$ Verlust, wo

$$l(y, y') = \sum_i (y_i - y_i')^2.$$

Wie wir später sehen werden, entspricht der $L_2$ Verlust der Annahme, dass unsere Daten durch Gaußschen Rauschen beschädigt wurden, während der $L_1$ Verlust einer Annahme von Rauschen aus einer Laplace-Verteilung entspricht.

#### Klassifizierung

Während Regressionsmodelle sich hervorragend eignen, um *wie viele zu adressieren?* Fragen, viele Probleme biegen sich nicht bequem zu dieser Vorlage. Beispielsweise möchte eine Bank der mobilen App Scheck-Scannen hinzufügen. Dies würde dazu führen, dass der Kunde ein Foto eines Schecks mit der Kamera seines Smartphones aufnimmt, und das Modell des maschinellen Lernens müsste in der Lage sein, den Text im Bild automatisch zu verstehen. Es müsste auch handgeschriebener Text verstehen, um noch robuster zu sein. Diese Art von System wird als optische Zeichenerkennung (Optical Character Recognition, OCR) bezeichnet, und die Art des Problems, das es behebt, heißt *Classification*. Es wird mit einem anderen Satz von Algorithmen behandelt als diejenigen, die für die Regression verwendet werden (obwohl viele Techniken übertragen werden).

In der Klassifizierung möchten wir, dass unser Modell einen Feature-Vektor betrachtet, z. B. die Pixelwerte in einem Bild, und dann vorhersagen, welche Kategorie (formell *Klassen* genannt), unter einigen (diskreten) Optionen, ein Beispiel gehört. Für handgeschriebene Ziffern können wir 10 Klassen haben, die den Ziffern 0 bis 9. entsprechen. Die einfachste Form der Klassifizierung ist, wenn es nur zwei Klassen gibt, ein Problem, das wir binäre Klassifikation nennen. Beispielsweise könnte unser Datensatz $X$ aus Bildern von Tieren bestehen, und unsere *Labels* $Y$ könnten die Klassen $\mathrm{\{cat, dog\}}$ sein. Während wir in der Regression einen *Regressor* suchten, um einen realen Wert $\hat{y}$ auszugeben, suchen wir in der Klassifikation einen *Klassifikator*, dessen Ausgabe $\hat{y}$ die prognostizierte Klassenzuweisung ist.

Aus Gründen, auf die wir eingehen werden, wenn das Buch technischer wird, kann es schwierig sein, ein Modell zu optimieren, das nur eine harte kategorische Zuweisung ausgeben kann, z.B. *cat* oder *dog*. In diesen Fällen ist es in der Regel viel einfacher, stattdessen unser Modell in der Sprache der Wahrscheinlichkeiten auszudrücken. Bei einem Beispiel $x$ weist unser Modell jedem Etikett $k$ eine Wahrscheinlichkeit $\hat{y}_k$ zu. Da es sich um Wahrscheinlichkeiten handelt, müssen sie positive Zahlen sein und bis zu $1$ addieren. Daher brauchen wir nur $K-1$ Zahlen, um Wahrscheinlichkeiten von $K$ Kategorien zuzuweisen. Dies ist für die binäre Klassifikation leicht zu sehen. Wenn es eine $0.6$ ($60\ %$) Wahrscheinlichkeit gibt, dass eine unfaire Münze Köpfe auftaucht, dann gibt es eine $0.4$ ($40\ %$) Wahrscheinlichkeit, dass sie Schwänze auftaucht. Wenn Sie zu unserem Beispiel für die Tierklassifizierung zurückkehren, kann ein Klassifikator ein Bild sehen und die Wahrscheinlichkeit ausgeben, dass es sich bei dem Bild um eine Katze $P(y=\text{cat} \mid x) = 0.9$ handelt. Wir können diese Zahl interpretieren, indem wir sagen, dass der Klassifikator $90\ %$ sicher ist, dass das Bild eine Katze darstellt. Das Ausmaß der Wahrscheinlichkeit für die vorhergesagte Klasse vermittelt einen Begriff der Unsicherheit. Es ist nicht der einzige Begriff der Unsicherheit, und wir werden andere in fortgeschritteneren Kapiteln diskutieren.

Wenn wir mehr als zwei mögliche Klassen haben, nennen wir das Problem *Multiclass-Klassifikation*. Häufige Beispiele sind die handgeschriebene Zeichenerkennung `[0, 1, 2, 3 ... 9, a, b, c, ...]`. Während wir Regressionsprobleme angegriffen haben, indem wir versuchen, die Verlustfunktionen $L_1$ oder $L_2$ zu minimieren, wird die gemeinsame Verlustfunktion für Klassifizierungsprobleme als Cross-Entropie bezeichnet.

Beachten Sie, dass die wahrscheinlichste Klasse nicht unbedingt diejenige ist, die Sie für Ihre Entscheidung verwenden werden. Angenommen, Sie finden diesen schönen Pilz in Ihrem Hinterhof, wie in :numref:`fig_death_cap` gezeigt.

![Death cap---do not eat!](../img/death_cap.jpg)
:width:`200px`
:label:`fig_death_cap`

Nehmen Sie nun an, Sie haben einen Klassifikator gebaut und trainiert, um vorherzusagen, ob ein Pilz giftig ist, basierend auf einem Foto. Nehmen wir an, dass unser Gift-Detektions-Klassifikator $P(y=\mathrm{death cap}|\mathrm{image}) = 0.2$ ausgibt. Mit anderen Worten, der Klassifikator ist $80\ %$ sicher, dass unser Pilz *nicht* eine Todeskappe ist. Trotzdem müssten Sie ein Narr sein, um es zu essen. Das liegt daran, dass der gewisse Vorteil eines köstlichen Abendessens kein $20\ %$ Risiko wert ist, daran zu sterben. Mit anderen Worten, die Wirkung des *unsicheren Risiko* überwiegt den Nutzen bei weitem. Wir können uns das formeller ansehen. Grundsätzlich müssen wir das erwartete Risiko berechnen, das wir eingehen, dh wir müssen die Wahrscheinlichkeit des Ergebnisses mit dem damit verbundenen Nutzen (oder Schaden) multiplizieren:

$$L(\mathrm{action}| x) = E_{y \sim p(y| x)}[\mathrm{loss}(\mathrm{action},y)].$$

Daher ist der Verlust $L$, der durch den Verzehr des Pilzes entstanden ist, $L(a=\mathrm{eat}| x) = 0.2 * \infty + 0.8 * 0 = \infty$, während die Kosten für die Entsorgung $L(a=\mathrm{discard}| x) = 0.2 * 0 + 0.8 * 1 = 0.8$ betragen.

Unsere Vorsicht war gerechtfertigt: Wie jeder Mykologe uns sagen würde, ist der obige Pilz tatsächlich ** eine Todeskappe. Die Klassifizierung kann viel komplizierter werden als nur die Binär-, Multiklass- oder sogar die Multi-Label-Klassifizierung. Zum Beispiel gibt es einige Varianten der Klassifizierung für die Adressierung von Hierarchien. Hierarchien gehen davon aus, dass es einige Beziehungen zwischen den vielen Klassen gibt. Also sind nicht alle Fehler gleich - wenn wir irren müssen, würden wir es vorziehen, eine verwandte Klasse anstatt eine entfernte Klasse falsch zu klassifizieren. In der Regel wird dies als *hierarchische Klassifizierung* bezeichnet. Ein frühes Beispiel ist auf [Linnaeus](https://en.wikipedia.org/wiki/Carl_Linnaeus) zurückzuführen, der die Tiere in einer Hierarchie organisierte.

Im Falle der Tierklassifizierung ist es vielleicht nicht so schlimm, einen Pudel mit einem Schnauzer zu verwechseln, aber unser Modell würde eine große Strafe zahlen, wenn es einen Pudel für einen Dinosaurier verwirrt. Welche Hierarchie relevant ist, hängt davon ab, wie Sie das Modell verwenden möchten. Zum Beispiel könnten Rasselschlangen und Strumpfschlangen in der Nähe des phylogenetischen Baumes liegen, aber das Verwechseln eines Rasselers mit einem Strumpfband könnte tödlich sein.

#### Tagging

Einige Klassifizierungsprobleme passen nicht ordentlich in die Binär- oder Multiclass-Klassifizierungs-Setups. Zum Beispiel könnten wir einen normalen binären Klassifikator trainieren, um Katzen von Hunden zu unterscheiden. Angesichts des aktuellen Zustands der Computer-Vision können wir dies einfach tun, mit Werkzeugen von Standardwerkzeugen. Dennoch, egal wie genau unser Modell wird, können wir uns in Schwierigkeiten befinden, wenn der Klassifikator auf ein Bild der Bremer Stadtmusiker trifft.

![A cat, a rooster, a dog and a donkey](../img/stackedanimals.jpg)
:width:`300px`

Wie Sie sehen können, gibt es eine Katze auf dem Bild und einen Hahn, einen Hund, einen Esel und einen Vogel, mit einigen Bäumen im Hintergrund. Abhängig davon, was wir letztlich mit unserem Modell machen wollen, ist die Behandlung dieses Problems als binäre Klassifizierungsproblem möglicherweise nicht sehr sinnvoll. Stattdessen möchten wir dem Modell die Möglichkeit geben, zu sagen, dass das Bild eine Katze darstellt *und* einen Hund *und* einen Esel
*und* ein Hahn *und* ein Vogel.

Das Problem des Lernens, Klassen vorherzusagen, die sind
*nicht gegenseitig ausschließlich* wird Multi-Label Klassifizierung genannt.
Probleme mit der automatischen Kennzeichnung werden in der Regel am besten als Multi-Label-Klassifizierungsprobleme beschrieben. Denken Sie an die Tags, die Personen auf Beiträge in einem Tech-Blog anwenden könnten, z. B. „Machine Learning“, „Technologie“, „Gadgets“, „Programmiersprachen“, „Linux“, „Cloud Computing“, „AWS“. Ein typischer Artikel kann 5-10 Tags angewendet werden, da diese Konzepte korreliert sind. Beiträge über „Cloud Computing“ werden wahrscheinlich „AWS“ erwähnen und Beiträge über „Machine Learning“ könnten sich auch mit „Programmiersprachen“ befassen.

Wir müssen uns auch mit dieser Art von Problem beschäftigen, wenn es um die biomedizinische Literatur geht, bei der Artikel richtig kennzeichnend ist, weil es Forschern erlaubt, umfassende Reviews der Literatur zu machen. In der National Library of Medicine gehen eine Reihe professioneller Annotatoren über jeden Artikel, der in PubMed indiziert wird, um ihn mit den relevanten Begriffen von MesH, einer Sammlung von etwa 28k Tags, zu verknüpfen. Dies ist ein zeitaufwändiger Prozess, und die Anmerkungen weisen in der Regel eine Verzögerung von einem Jahr zwischen Archivierung und Tagging auf. Maschinelles Lernen kann hier verwendet werden, um vorläufige Tags bereitzustellen, bis jeder Artikel eine ordnungsgemäße manuelle Überprüfung haben kann. In der Tat hat die BioASQ-Organisation seit mehreren Jahren [hosted a competition](http://bioasq.org/), um genau dies zu tun.

#### Suche und Ranking

Manchmal wollen wir nicht nur jedes Beispiel einem Bucket oder einem realen Wert zuweisen. Im Bereich des Informationsabrufs möchten wir einer Reihe von Elementen ein Ranking auferlegen. Nehmen wir zum Beispiel die Websuche, ist das Ziel weniger zu bestimmen, ob eine bestimmte Seite für eine Abfrage relevant ist, sondern, welche der Fülle von Suchergebnissen für einen bestimmten Benutzer *am relevantesten ist*. Wir kümmern uns wirklich um die Reihenfolge der relevanten Suchergebnisse und unser Lernalgorithmus muss geordnete Teilmengen von Elementen aus einem größeren Satz erzeugen. Mit anderen Worten, wenn wir aufgefordert werden, die ersten 5 Buchstaben aus dem Alphabet zu produzieren, gibt es einen Unterschied zwischen der Rückgabe von ``A B C D E`` and ``C A B E D``. Selbst wenn die Ergebnismenge identisch ist, ist die Reihenfolge innerhalb des Satzes wichtig.

Eine mögliche Lösung für dieses Problem besteht darin, zuerst jedem Element in der Menge eine entsprechende Relevanzbewertung zuzuweisen und dann die bestbewerteten Elemente abzurufen. [PageRank](https://en.wikipedia.org/wiki/PageRank), die ursprüngliche geheime Sauce hinter der Google-Suchmaschine war ein frühes Beispiel für ein solches Scoring-System, aber es war merkwürdig, dass es tat hängt nicht von der tatsächlichen Abfrage ab. Hier stützten sie sich auf einen einfachen Relevanzfilter, um die Gruppe der relevanten Elemente zu identifizieren, und dann auf PageRank, um die Ergebnisse zu sortieren, die den Abfragebegriff enthalten. Heutzutage verwenden Suchmaschinen maschinelles Lernen und Verhaltensmodelle, um abfrage-abhängige Relevanzwerte zu erhalten. Es gibt ganze akademische Konferenzen, die sich diesem Thema widmen.

#### Empfehlungssysteme
:label:`subsec_recommender_systems`

Empfehlungssysteme sind eine weitere Problemeinstellung, die mit der Suche und dem Ranking zusammenhängt. Die Probleme sind insofern ähnlich, als das Ziel darin besteht, dem Benutzer eine Reihe relevanter Elemente anzuzeigen. Der Hauptunterschied besteht in der Betonung auf *Personalisierung* für bestimmte Benutzer im Kontext von Empfehlungssystemen. Zum Beispiel können bei Filmempfehlungen die Ergebnisseite für einen SciFi Fan und die Ergebnisseite für einen Kenner von Peter Sellers Komödien erheblich abweichen. Ähnliche Probleme treten in anderen Empfehlungseinstellungen auf, z. B. für Einzelhandelsprodukte, Musik oder Nachrichtenempfehlungen.

In einigen Fällen geben Kunden explizites Feedback ab, in dem mitgeteilt wird, wie sehr ihnen ein bestimmtes Produkt gefallen hat (z. B. die Produktbewertungen und -bewertungen bei Amazon, IMDB, GoodReads usw.). In einigen anderen Fällen liefern sie implizites Feedback, z. B. durch Überspringen von Titeln auf einer Playlist, was auf Unzufriedenheit hindeutet, aber nur darauf hindeutet, dass der Song im Kontext unangemessen war. In den einfachsten Formulierungen sind diese Systeme geschult, um einige Punktzahl $y_{ij}$ zu schätzen, wie eine geschätzte Bewertung oder die Wahrscheinlichkeit des Kaufs, gegeben ein Benutzer $u_i$ und Produkt $p_j$.

Angesichts eines solchen Modells könnten wir für jeden bestimmten Benutzer den Satz von Objekten mit den größten Punktzahlen $y_{ij}$ abrufen, was dann dem Kunden empfohlen werden könnte. Produktionssysteme sind wesentlich fortgeschrittener und berücksichtigen bei der Berechnung solcher Werte detaillierte Benutzeraktivitäten und Artikelmerkmale. :numref:`fig_deeplearning_amazon` ist ein Beispiel für Deep Learning-Bücher, die von Amazon auf der Grundlage von Personalisierungsalgorithmen empfohlen werden, die auf die Präferenzen des Autors abgestimmt sind.

![Deep learning books recommended by Amazon.](../img/deeplearning_amazon.jpg)
:label:`fig_deeplearning_amazon`

Trotz ihres enormen wirtschaftlichen Werts leiden Empfehlungssysteme, die naiv auf Vorhersagemodellen aufbauen, unter gravierenden konzeptionellen Fehlern. Zunächst beobachten wir nur *zensiertes Feedback*. Benutzer bewerten bevorzugt Filme, von denen sie sich stark fühlen: Sie werden feststellen, dass Artikel viele 5- und 1-Sterne-Bewertungen erhalten, aber dass es auffällig wenige 3-Sterne-Bewertungen gibt. Darüber hinaus sind aktuelle Kaufgewohnheiten oft das Ergebnis des derzeit vorhandenen Empfehlungsalgorithmus, aber Lernalgorithmen berücksichtigen dieses Detail nicht immer. So ist es möglich, dass sich Feedback-Schleifen bilden, wo ein Empfehlungssystem bevorzugt einen Artikel schiebt, der dann als besser (aufgrund größerer Einkäufe) genommen wird und wiederum noch häufiger empfohlen wird. Viele dieser Probleme, wie man mit Zensur, Anreizen und Feedbackschleifen umgeht, sind wichtige offene Forschungsfragen.

#### Sequenz-Lernen

Bisher haben wir Probleme untersucht, bei denen wir eine feste Anzahl von Eingängen haben und eine feste Anzahl von Ausgängen erzeugen. Bevor wir überlegten, die Preise von einer festen Reihe von Funktionen vorherzusagen: Quadratmeterzahl, Anzahl der Schlafzimmer, Anzahl der Badezimmer, Gehzeit in die Innenstadt. Wir diskutierten auch das Mapping von einem Bild (mit fester Dimension) zu den prognostizierten Wahrscheinlichkeiten, dass es zu jeder bestimmten Anzahl von Klassen gehört, oder das Nehmen einer Benutzer-ID und einer Produkt-ID sowie die Vorhersage einer Sternbewertung. In diesen Fällen, sobald wir unsere Eingabe mit fester Länge in das Modell einspeisen, um eine Ausgabe zu erzeugen, vergisst das Modell sofort, was es gerade gesehen hat.

Das könnte in Ordnung sein, wenn unsere Eingaben wirklich alle die gleichen Dimensionen haben und wenn aufeinanderfolgende Eingaben wirklich nichts miteinander zu tun haben. Aber wie würden wir mit Video-Snippets umgehen? In diesem Fall kann jedes Snippet aus einer anderen Anzahl von Frames bestehen. Und unsere Vermutung, was in jedem Frame vor sich geht, könnte viel stärker sein, wenn wir die vorherigen oder nachfolgenden Frames berücksichtigen. Dasselbe gilt für die Sprache. Ein beliebtes Deep Learning-Problem ist die maschinelle Übersetzung: die Aufgabe, Sätze in einer Quellsprache zu nehmen und ihre Übersetzung in einer anderen Sprache vorherzusagen.

Diese Probleme treten auch in der Medizin auf. Wir möchten vielleicht ein Modell, um Patienten auf der Intensivstation zu überwachen und Alarme abzugeben, wenn ihr Todesrisiko in den nächsten 24 Stunden einen Schwellenwert überschreitet. Wir würden definitiv nicht wollen, dass dieses Modell jede Stunde alles wegwirft, was es über die Patientengeschichte weiß, und nur seine Vorhersagen basierend auf den neuesten Messungen.

Diese Probleme gehören zu den spannendsten Anwendungen des maschinellen Lernens und sind Instanzen von „Sequence Learning*“. Sie benötigen ein Modell, um entweder Sequenzen von Eingängen aufzunehmen oder Sequenzen von Ausgängen (oder beides!) zu emittieren. Diese letzteren Probleme werden manchmal als „`seq2seq`` problems.  Language translation is a ``seq2seq` „Problem bezeichnet. Das Transkribieren von Text aus der gesprochenen Rede ist auch ein ``seq2seq`` Problem. Während es unmöglich ist, alle Arten von Sequenztransformationen zu berücksichtigen, sind eine Reihe von Sonderfällen erwähnenswert:

**Tagging und Parsing**. Hierbei handelt es sich um eine Textsequenz mit Attributen.
Mit anderen Worten, die Anzahl der Eingänge und Ausgänge ist im Wesentlichen gleich. Zum Beispiel möchten wir vielleicht wissen, wo die Verben und Subjekte sind. Alternativ möchten wir vielleicht wissen, welche Wörter die benannten Entitäten sind. Im Allgemeinen ist das Ziel, Text basierend auf strukturellen und grammatischen Annahmen zu zerlegen und zu kommentieren, um eine Anmerkung zu erhalten. Das klingt komplexer, als es tatsächlich ist. Im Folgenden finden Sie ein sehr einfaches Beispiel für die Anmerkungen eines Satzes mit Tags, die angeben, welche Wörter sich auf benannte Entitäten beziehen.

```text
Tom has dinner in Washington with Sally.
Ent  -    -    -     Ent      -    Ent
```

**Automatische Spracherkennung**. Mit Spracherkennung, die Eingabesequenz $x$
ist eine Audioaufnahme eines Lautsprechers (dargestellt in :numref:`fig_speech`), und der Ausgang $y$ ist das Textprotokoll dessen, was der Sprecher sagte. Die Herausforderung besteht darin, dass es viel mehr Audio-Frames gibt (Ton wird typischerweise mit 8 kHz oder 16 kHz gesampelt) als Text, d.h. es gibt keine 1:1 -Korrespondenz zwischen Audio und Text, da Tausende von Samples einem einzigen gesprochenen Wort entsprechen. Dies sind ``seq2seq`` Probleme, bei denen die Ausgabe viel kürzer ist als die Eingabe.

![`-D-e-e-p- L-ea-r-ni-ng-`](../img/speech.png)
:width:`700px`
:label:`fig_speech`

**Text zur Rede**. Text-to-Speech (TTS) ist die Umkehrung der Spracherkennung.
Mit anderen Worten, die Eingabe $x$ ist Text und die Ausgabe $y$ ist eine Audiodatei. In diesem Fall ist die Ausgabe *viel länger* als die Eingabe. Obwohl es für *menschen* leicht ist, eine schlechte Audiodatei zu erkennen, ist dies für Computer nicht ganz so trivial.

**Maschinenübersetzung**. Im Gegensatz zum Fall der Spracherkennung, wo entsprechende
Eingänge und Ausgänge treten in der gleichen Reihenfolge auf (nach der Ausrichtung), in der maschinellen Übersetzung kann die Umkehrung der Reihenfolge von entscheidender Bedeutung sein. Mit anderen Worten, während wir immer noch eine Sequenz in eine andere umwandeln, wird weder die Anzahl der Eingänge und Ausgänge noch die Reihenfolge der entsprechenden Datenpunkte als gleich angenommen. Betrachten Sie das folgende illustrative Beispiel für die eigentümliche Tendenz der Deutschen, die Verben am Ende der Sätze zu platzieren.

```text
German:           Haben Sie sich schon dieses grossartige Lehrwerk angeschaut?
English:          Did you already check out this excellent tutorial?
Wrong alignment:  Did you yourself already this excellent tutorial looked-at?
```

Viele verwandte Probleme tauchen in anderen Lernaufgaben auf. Zum Beispiel ist die Bestimmung der Reihenfolge, in der ein Benutzer eine Webseite liest, ein zweidimensionales Layoutanalyseproblem. Dialogprobleme weisen alle Arten von zusätzlichen Komplikationen auf, bei denen die Bestimmung, was als nächstes gesagt werden soll, die Berücksichtigung des realen Wissens und des vorherigen Zustands des Gesprächs über lange zeitliche Entfernungen erfordert. Dies ist ein aktiver Forschungsbereich.

### Unbeaufsichtigtes Lernen

Alle bisherigen Beispiele beziehen sich auf *Supervised Learning*, d.h. Situationen, in denen wir dem Modell ein riesiges Dataset füttern, das sowohl die Features als auch die entsprechenden Zielwerte enthält. Man könnte sich vorstellen, dass der betreute Lernende einen extrem spezialisierten Job und einen extrem analen Chef hat. Der Chef steht über deiner Schulter und sagt dir genau, was in jeder Situation zu tun ist, bis du lernst, von Situationen zu Aktionen abzubilden. Für solch einen Boss zu arbeiten klingt ziemlich lahm. Auf der anderen Seite ist es leicht, diesem Chef zu gefallen. Sie erkennen nur das Muster so schnell wie möglich und imitieren ihre Handlungen.

In einer völlig entgegengesetzten Weise könnte es frustrierend sein, für einen Chef zu arbeiten, der keine Ahnung hat, was er von dir will. Wenn Sie jedoch planen, Datenwissenschaftler zu sein, sollten Sie sich daran gewöhnen. Der Chef könnte Ihnen einfach eine riesige Dump von Daten geben und Ihnen sagen, * etwas Data Science damit zu machen!* Das klingt vage, weil es so ist. Wir nennen diese Problemklasse *unbeaufsichtigtes Lernen*, und die Art und Anzahl der Fragen, die wir stellen können, ist nur durch unsere Kreativität begrenzt. Wir werden eine Reihe von unbeaufsichtigten Lerntechniken in späteren Kapiteln ansprechen. Um Ihren Appetit für jetzt zu wecken, beschreiben wir einige der Fragen, die Sie stellen könnten:

* Können wir eine kleine Anzahl von Prototypen finden?
, die die Daten genau zusammenfassen? Können wir sie anhand einer Reihe von Fotos in Landschaftsfotos, Bilder von Hunden, Babys, Katzen, Berggipfeln usw. gruppieren? Ebenso können wir sie angesichts einer Sammlung von Surfaktivitäten der Benutzer in Benutzer mit ähnlichem Verhalten gruppieren? Dieses Problem wird üblicherweise als *clustering* bezeichnet.
* Können wir eine kleine Anzahl von Parametern finden
die die relevanten Eigenschaften der Daten genau erfassen? Die Flugbahnen eines Balls sind ziemlich gut durch Geschwindigkeit, Durchmesser und Masse des Balls beschrieben. Schneider haben eine kleine Anzahl von Parametern entwickelt, die die menschliche Körperform ziemlich genau zum Zweck der Anpassung von Kleidung beschreiben. Diese Probleme werden als *Subraumschätzung* Probleme bezeichnet. Wenn die Abhängigkeit linear ist, wird sie *Hauptkomponentenanalyse* genannt.
* Gibt es eine Darstellung von (beliebig strukturierten) Objekten
im euklidischen Raum (dh der Raum der Vektoren in $\mathbb{R}^n$), so dass symbolische Eigenschaften gut abgestimmt werden können? Dies wird *Repräsentationslernen* genannt und wird verwendet, um Entitäten und ihre Beziehungen zu beschreiben, wie Rom $-$ Italien $+$ Frankreich $=$ Paris.
* Gibt es eine Beschreibung der Hauptursachen
von einem Großteil der Daten, die wir beobachten? Wenn wir zum Beispiel demographische Daten über Immobilienpreise, Umweltverschmutzung, Kriminalität, Standort, Bildung, Gehälter usw. haben, können wir herausfinden, wie diese einfach auf empirischen Daten zusammenhängen? Die Felder, die sich mit *Kausalität* und
*probabilistische grafische Modelle* adressieren dieses Problem.
* Eine weitere wichtige und spannende jüngste Entwicklung im unbeaufsichtigten Lernen
ist das Aufkommen von *generativen gegnerischen Netzwerken* (GaNs). Diese geben uns eine prozedurale Möglichkeit, Daten zu synthetisieren, sogar komplizierte strukturierte Daten wie Bilder und Audio. Die zugrunde liegenden statistischen Mechanismen sind Tests, um zu überprüfen, ob reale und gefälschte Daten identisch sind. Wir werden ihnen ein paar Notizbücher widmen.

### Interaktion mit einer Umgebung

Bisher haben wir nicht diskutiert, woher Daten tatsächlich kommen oder was tatsächlich *passiert*, wenn ein Machine Learning-Modell eine Ausgabe generiert. Das liegt daran, dass überwachtes Lernen und unbeaufsichtigtes Lernen diese Probleme nicht auf sehr anspruchsvolle Weise ansprechen. In beiden Fällen greifen wir im Voraus einen großen Datenhaufen und setzen dann unsere Mustererkennungsmaschinen in Bewegung, ohne jemals wieder mit der Umgebung zu interagieren. Da das gesamte Lernen stattfindet, nachdem der Algorithmus von der Umgebung getrennt wurde, wird dies manchmal als „Offline-Lernen*“ bezeichnet. Für überwachtes Lernen sieht der Prozess wie :numref:`fig_data_collection` aus.

![Collect data for supervised learning from an environment.](../img/data-collection.svg)
:label:`fig_data_collection`

Diese Einfachheit des Offline-Lernens hat seinen Charme. Der Vorteil ist, dass wir uns um die Mustererkennung isoliert kümmern können, ohne von diesen anderen Problemen ablenken zu müssen. Aber der Nachteil ist, dass die Problemformulierung ziemlich einschränkend ist. Wenn Sie ehrgeiziger sind oder wenn Sie die Roboter-Serie von Asimov lesen, dann könnten Sie sich künstlich intelligente Bots vorstellen, die nicht nur Vorhersagen machen, sondern auch Aktionen in der Welt ergreifen können. Wir wollen an intelligente *Agenten* denken, nicht nur prädiktive *Modelle*. Das bedeutet, dass wir darüber nachdenken müssen, *Aktionen* zu wählen, nicht nur *Vorhersage* zu machen. Im Gegensatz zu Vorhersagen wirken sich Aktionen auch tatsächlich auf die Umwelt aus. Wenn wir einen intelligenten Agenten ausbilden wollen, müssen wir berücksichtigen, wie seine Handlungen die zukünftigen Beobachtungen des Agenten beeinflussen könnten.

Wenn man die Interaktion mit einer Umgebung betrachtet, öffnet sich eine ganze Reihe neuer Modellierungsfragen. Ist die Umgebung:

* Erinnerst du dich daran, was wir vorher getan haben?
* Möchten Sie uns helfen, z.B. einem Benutzer, der Text in einen Spracherkenner liest?
* Willst du uns schlagen, also eine gegnerische Einstellung wie Spam-Filterung (gegen Spammer) oder ein Spiel spielen (gegen einen Gegner)?
* Nicht kümmern (wie in vielen Fällen)?
* Haben sich verändernde Dynamik (ähneln zukünftige Daten immer der Vergangenheit oder ändern sich die Muster im Laufe der Zeit, entweder natürlich oder als Reaktion auf unsere automatisierten Tools)?

Diese letzte Frage wirft das Problem von *Distribution Shift* auf (wenn Trainings- und Testdaten unterschiedlich sind). Es ist ein Problem, das die meisten von uns erlebt haben, wenn sie Prüfungen von einem Dozenten ablegen, während die Hausaufgaben von ihren TAs komponiert wurden. Wir werden kurz Verstärkungslernen und gegnerisches Lernen beschreiben, zwei Einstellungen, die explizit die Interaktion mit einer Umgebung berücksichtigen.

### Verstärkung Lernen

Wenn Sie daran interessiert sind, maschinelles Lernen zu verwenden, um einen Agenten zu entwickeln, der mit einer Umgebung interagiert und Maßnahmen ergreift, dann werden Sie sich wahrscheinlich auf *Verstärkungslernen* (RL) konzentrieren. Dies kann Anwendungen für Robotik, Dialogsysteme und sogar die Entwicklung von KI für Videospiele umfassen.
*Tiefenverstärkung* (DRL), das gilt
tiefe neuronale Netze zu RL-Problemen, hat an Popularität angestiegen. Der Durchbruch [deep Q-network that beat humans at Atari games using only the visual input](https://www.wired.com/2015/02/google-ai-plays-atari-like-pros/) und der [AlphaGo program that dethroned the world champion at the board game Go](https://www.wired.com/2017/05/googles-alphago-trounces-humans-also-gives-boost/) sind zwei prominente Beispiele.

Verstärkungslernen gibt eine sehr allgemeine Aussage eines Problems, bei dem ein Agent mit einer Umgebung über eine Reihe von Zeitstempen* interagiert. Zu jedem Zeitstempel $t$ erhält der Agent eine Beobachtung $o_t$ aus der Umgebung und muss eine Aktion $a_t$ wählen, die anschließend über einen Mechanismus (manchmal als Aktor bezeichnet) an die Umgebung zurückübertragen wird. Schließlich erhält der Agent eine Belohnung $r_t$ von der Umwelt. Der Agent erhält dann eine nachfolgende Beobachtung und wählt eine nachfolgende Aktion aus usw. Das Verhalten eines RL-Agenten unterliegt einer *Policy*. Kurz gesagt, eine *Policy* ist nur eine Funktion, die von Beobachtungen (der Umgebung) zu Aktionen abbildet. Das Ziel der Verstärkung des Lernens ist es, eine gute Politik zu entwickeln.

![The interaction between reinforcement learning and an environment.](../img/rl-environment.svg)

Es ist schwer, die Allgemeinheit des RL-Rahmens zu überschreiben. Zum Beispiel können wir jedes überwachte Lernproblem als RL-Problem umwandeln. Angenommen, wir hatten ein Klassifizierungsproblem. Wir könnten einen RL-Agenten mit einer *Aktion* erstellen, die jeder Klasse entspricht. Wir könnten dann eine Umgebung schaffen, die eine Belohnung gab, die genau der Verlustfunktion aus dem ursprünglichen überwachten Problem entspricht.

Abgesehen davon kann RL auch viele Probleme ansprechen, die überwachtes Lernen nicht möglich ist. Zum Beispiel erwarten wir beim überwachten Lernen immer, dass der Trainingseinput mit dem richtigen Etikett verknüpft ist. Aber in RL gehen wir nicht davon aus, dass uns die Umwelt für jede Beobachtung die optimale Aktion sagt. Im Allgemeinen bekommen wir nur eine Belohnung. Darüber hinaus kann uns die Umwelt nicht einmal sagen, welche Aktionen zur Belohnung geführt haben.

Betrachten Sie zum Beispiel das Schachspiel. Das einzige wirkliche Belohnungssignal kommt am Ende des Spiels, wenn wir entweder gewinnen, was wir eine Belohnung von 1 zuweisen können, oder wenn wir verlieren, was wir eine Belohnung von -1 zuweisen können. Daher müssen sich die Verstärkungslernenden mit dem *Kreditzuweisungsproblem* befassen: Bestimmen, welche Aktionen gutschreiben oder für ein Ergebnis verantwortlich gemacht werden sollen. Das gleiche gilt für einen Mitarbeiter, der am 11. Oktober eine Beförderung erhält. Diese Förderung spiegelt wahrscheinlich eine große Anzahl gut ausgewählter Maßnahmen im Vorjahr wider. Um mehr Werbeaktionen in der Zukunft zu erhalten, müssen Sie herausfinden, welche Aktionen auf dem Weg zur Promotion geführt haben.

Verstärkungslernende müssen sich möglicherweise auch mit dem Problem der teilweisen Beobachtbarkeit befassen. Das heißt, die aktuelle Beobachtung sagt Ihnen möglicherweise nicht alles über Ihren aktuellen Zustand. Angenommen, ein Reinigungsroboter fand sich in einem von vielen identischen Schränken in einem Haus gefangen. Der genaue Standort (und damit der Zustand) des Roboters kann es erforderlich sein, seine bisherigen Beobachtungen vor dem Betreten des Schrankes zu berücksichtigen.

Schließlich wissen die Verstärkungslerner zu einem bestimmten Zeitpunkt vielleicht von einer guten Politik, aber es könnte viele andere bessere Politiken geben, die der Agent nie versucht hat. Der Verstärkungslernende muss sich ständig entscheiden, ob er die beste derzeit bekannte Strategie als Politik ausnutzt oder den Raum der Strategien erkundet* und potenziell kurzfristige Belohnungen im Austausch für Wissen aufgibt.

#### MDPs, Banditen und Freunde

Das allgemeine Problem des Verstärkungslernens ist eine sehr allgemeine Einstellung. Aktionen wirken sich auf nachfolgende Beobachtungen aus. Belohnungen werden nur entsprechend den ausgewählten Aktionen beobachtet. Die Umwelt kann entweder vollständig oder teilweise beobachtet werden. Die Berücksichtigung all dieser Komplexität auf einmal kann zu viel von Forschern verlangen. Darüber hinaus weist nicht jedes praktische Problem diese ganze Komplexität auf. Als Ergebnis haben die Forscher eine Reihe von untersucht
*Sonderfallen* von Verstärkungslernproblemen.

Wenn die Umwelt vollständig beobachtet wird, nennen wir das RL-Problem ein *Markov Decision Process* (MDP). Wenn der Zustand nicht von den vorherigen Aktionen abhängt, nennen wir das Problem ein *kontextbezogenes Banditenproblem*. Wenn es keinen Zustand gibt, nur eine Reihe von verfügbaren Aktionen mit anfänglich unbekannten Belohnungen, ist dieses Problem das klassische *mehrarmige Banditenproblem*.

## Wurzeln

Obwohl viele Deep Learning-Methoden jüngste Erfindungen sind, haben Menschen seit Jahrhunderten den Wunsch gehalten, Daten zu analysieren und zukünftige Ergebnisse vorherzusagen. In der Tat hat ein Großteil der Naturwissenschaft seine Wurzeln darin. Zum Beispiel ist die Bernoulli-Verteilung benannt nach [Jacob Bernoulli (1655-1705)](https://en.wikipedia.org/wiki/Jacob_Bernoulli), und die Gaußsche Verteilung wurde von [Carl Friedrich Gauß (1777-1855)](https://en.wikipedia.org/wiki/Carl_Friedrich_Gauss) entdeckt. Er erfand zum Beispiel den kleinsten Mittelquadrat-Algorithmus, der heute noch für unzählige Probleme von der Versicherungsberechnung bis zur medizinischen Diagnostik verwendet wird. Diese Werkzeuge führten zu einem experimentellen Ansatz in den Naturwissenschaften — zum Beispiel wird das Ohmsche Gesetz über Strom und Spannung in einem Widerstand durch ein lineares Modell perfekt beschrieben.

Schon im Mittelalter hatten Mathematiker eine scharfste Intuition von Schätzungen. So veranschaulicht das Geometrie-Buch von [Jacob Köbel (1460-1533)](https://www.maa.org/press/periodicals/convergence/mathematical-treasures-jacob-kobels-geometry) die durchschnittliche Länge von 16 erwachsenen Männerfüßen, um die durchschnittliche Fußlänge zu erhalten.

![Estimating the length of a foot](../img/koebel.jpg)
:width:`500px`
:label:`fig_koebel`

:numref:`fig_koebel` veranschaulicht, wie dieser Schätzer funktioniert. Die 16 erwachsenen Männer wurden gebeten, sich hintereinander zu stellen, als sie die Kirche verlassen. Ihre Gesamtlänge wurde dann durch 16 geteilt, um eine Schätzung für das zu erhalten, was jetzt 1 Fuß beträgt. Dieser „Algorithmus“ wurde später verbessert, um mit falsch geformten Füßen umzugehen — die 2 Männer mit den kürzesten und längsten Füßen wurden weggeschickt, wobei der Durchschnitt nur über den Rest lag. Dies ist eines der frühesten Beispiele für die getrimmte Mittelschätzung.

Statistiken haben mit der Sammlung und Verfügbarkeit von Daten wirklich abgenommen. Einer seiner Titanen, [Ronald Fisher (1890-1962)](https://en.wikipedia.org/wiki/Ronald_Fisher), trug wesentlich zu seiner Theorie und auch zu seinen Anwendungen in der Genetik bei. Viele seiner Algorithmen (wie Linear Diskriminant Analysis) und Formel (wie die Fisher Information Matrix) werden heute noch häufig verwendet (selbst das Iris-Dataset, das er 1936 veröffentlicht hat, wird manchmal noch verwendet, um Machine Learning Algorithmen zu illustrieren). Fisher war auch ein Befürworter der Eugenik, die uns daran erinnern sollte, dass die moralisch zweifelhafte Nutzung der Datenwissenschaft eine so lange und anhaltende Geschichte hat wie ihre produktive Nutzung in der Industrie und den Naturwissenschaften.

Ein zweiter Einfluss für maschinelles Lernen kam aus der Informationstheorie [(Claude Shannon, 1916-2001)](https://en.wikipedia.org/wiki/Claude_Shannon) und der Rechentheorie über [Alan Turing (1912-1954)](https://en.wikipedia.org/wiki/Alan_Turing). Turing stellte die Frage: „Können Maschinen denken?“ in seiner berühmten Zeitung [Computing machinery and intelligence](https://en.wikipedia.org/wiki/Computing_Machinery_and_Intelligence) (Mind, Oktober 1950). In dem, was er als Turing-Test beschrieben hat, kann eine Maschine als intelligent angesehen werden, wenn es für einen menschlichen Evaluator schwierig ist, zwischen den Antworten einer Maschine und einem Menschen basierend auf textuellen Interaktionen zu unterscheiden.

Ein weiterer Einfluss kann in der Neurowissenschaft und Psychologie gefunden werden. Schließlich zeigen Menschen eindeutig intelligentes Verhalten. Es ist also nur vernünftig, sich zu fragen, ob man diese Kapazität erklären und möglicherweise zurückentwickeln könnte. Einer der ältesten Algorithmen, die auf diese Weise inspiriert wurden, wurde von [Donald Hebb (1904-1985)](https://en.wikipedia.org/wiki/Donald_O._Hebb) formuliert. In seinem bahnbrechenden Buch The Organization of Behavior :cite:`Hebb.Hebb.1949` stellte er fest, dass Neuronen durch positive Verstärkung lernen. Dies wurde als die hebbische Lernregel bekannt. Es ist der Prototyp des Perzeptron-Lernalgorithmus von Rosenblatt und er legte die Grundlagen vieler stochastischer Gradientenabstiegsalgorithmen, die heute Deep Learning untermauern: Bevorzugtes Verhalten verstärken und unerwünschtes Verhalten verringern, um gute Einstellungen der Parameter in einem neuronalen Netzwerk zu erhalten.

Biologische Inspiration hat *neuronale Netzwerke* ihren Namen gegeben. Seit mehr als einem Jahrhundert (aus den Modellen von Alexander Bain, 1873 und James Sherrington, 1890) haben Forscher versucht, Rechenschaltungen zusammenzustellen, die Netzwerken interagierender Neuronen ähneln. Im Laufe der Zeit ist die Interpretation der Biologie weniger wörtlich geworden, aber der Name blieb fest. Im Herzen liegen einige Schlüsselprinzipien, die heute in den meisten Netzwerken zu finden sind:

* Der Wechsel von linearen und nichtlinearen Verarbeitungseinheiten, oft als *Layers* bezeichnet.
* Die Verwendung der Kettenregel (auch bekannt als *Backpropagation*) zur gleichmaligen Anpassung von Parametern im gesamten Netzwerk.

Nach anfänglichen raschen Fortschritten schmachten die Forschung in neuronalen Netzen von etwa 1995 bis 2005. Dies war auf eine Reihe von Gründen zurückzuführen. Das Training eines Netzwerks ist rechnerisch sehr teuer. Während RAM am Ende des vergangenen Jahrhunderts reichlich vorhanden war, war die Rechenleistung knapp. Zweitens waren Datensätze relativ klein. Tatsächlich war der Iris-Datensatz von Fisher aus dem Jahr 1932 ein beliebtes Werkzeug, um die Wirksamkeit von Algorithmen zu testen. MNIST mit seinen 60.000 handgeschriebenen Ziffern galt als riesig.

Angesichts der Knappheit an Daten und Berechnungen erwiesen sich starke statistische Werkzeuge wie Kernel-Methoden, Entscheidungsbäume und grafische Modelle als empirisch überlegen. Im Gegensatz zu neuronalen Netzen brauchten sie keine Wochen, um zu trainieren und lieferten vorhersehbare Ergebnisse mit starken theoretischen Garantien.

## Der Weg zum Deep Learning

Vieles änderte sich mit der Bereitschaft großer Datenmengen, aufgrund des World Wide Web, der Einführung von Unternehmen, die Hunderte Millionen von Nutzern online bedienen, einer Verbreitung von billigen, qualitativ hochwertigen Sensoren, billigen Datenspeicherung (Kryder's Gesetz) und billigen Berechnungen (Moore's Gesetz), insbesondere in der Form von GPUs, ursprünglich für Computer-Gaming entwickelt. Plötzlich wurden Algorithmen und Modelle, die rechnerisch nicht machbar schienen, relevant (und umgekehrt). Dies ist am besten in :numref:`tab_intro_decade` illustriert.

:Dataset vs. Computerspeicher und Rechenleistung

|Decade|Dataset|Memory|Floating Point Calculations per Second|
|:--|:-|:-|:-|
|1970|100 (Iris)|1 KB|100 KF (Intel 8080)|
|1980|1 K (House prices in Boston)|100 KB|1 MF (Intel 80186)|
|1990|10 K (optical character recognition)|10 MB|10 MF (Intel 80486)|
|2000|10 M (web pages)|100 MB|1 GF (Intel Core)|
|2010|10 G (advertising)|1 GB|1 TF (Nvidia C2050)|
|2020|1 T (social network)|100 GB|1 PF (Nvidia DGX-2)|
:label:`tab_intro_decade`

Es ist offensichtlich, dass RAM mit dem Wachstum der Daten nicht Schritt gehalten hat. Gleichzeitig hat der Anstieg der Rechenleistung die der verfügbaren Daten übertroffen. Dies bedeutet, dass statistische Modelle speichereffizienter werden mussten (dies wird normalerweise durch Hinzufügen von Nichtlinearitäten erreicht) und gleichzeitig aufgrund eines erhöhten Rechenbudgets mehr Zeit für die Optimierung dieser Parameter aufwenden konnten. Folglich bewegte sich der Sweet Spot im maschinellen Lernen und Statistiken von (generalisierten) linearen Modellen und Kernelmethoden zu tiefen Netzwerken. Dies ist auch einer der Gründe, warum viele der Grundpfeiler von Deep Learning, wie mehrschichtige Wahrnehmungen :cite:`McCulloch.Pitts.1943`, faltete neuronale Netze :cite:`LeCun.Bottou.Bengio.ea.1998`, Long Short-Speicher :cite:`Hochreiter.Schmidhuber.1997` und Q-Learning :cite:`Watkins.Dayan.1992`, im letzten Jahrzehnt im Wesentlichen „wiederentdeckt“ wurden, nachdem sie vergleichsweise ruhend für beträchtliche Zeit.

Die jüngsten Fortschritte bei statistischen Modellen, Anwendungen und Algorithmen wurden manchmal mit der Kambrischen Explosion verglichen: ein Moment rascher Fortschritte bei der Entwicklung der Arten. Tatsächlich ist der Stand der Technik nicht nur eine bloße Folge der verfügbaren Ressourcen, die auf jahrzehntelange Algorithmen angewendet werden. Beachten Sie, dass die Liste unten kaum kratzt die Oberfläche der Ideen, die Forschern geholfen haben, enorme Fortschritte in den letzten zehn Jahren zu erzielen.

* Neuartige Methoden zur Kapazitätssteuerung, wie Dropout :cite:`Srivastava.Hinton.Krizhevsky.ea.2014`, haben dazu beigetragen, die Gefahr einer Überrüstung zu mindern. Dies wurde durch die Anwendung der Rauschinjektion :cite:`Bishop.1995` im gesamten Netzwerk erreicht, wobei Gewichte für Trainingszwecke durch Zufallsvariablen ersetzt wurden.
* Aufmerksamkeitsmechanismen lösten ein zweites Problem, das die Statistik seit über einem Jahrhundert geplagt hatte: wie man den Speicher und die Komplexität eines Systems erhöhen kann, ohne die Anzahl der erlernbaren Parameter zu erhöhen. :cite:`Bahdanau.Cho.Bengio.2014` fand eine elegante Lösung, indem man nur als lernbare Zeigerstruktur betrachtet. Anstatt sich an einen ganzen Satz erinnern zu müssen, z. B. für maschinelle Übersetzung in einer festdimensionalen Darstellung, war alles, was gespeichert werden musste, ein Zeiger auf den Zwischenzustand des Übersetzungsprozesses. Dies ermöglichte eine deutlich erhöhte Genauigkeit für lange Sätze, da sich das Modell nicht mehr an den gesamten Satz erinnern musste, bevor es mit der Generierung eines neuen Satzes begonnen hat.
* Mehrstufige Designs, z.B. über die MemNets (MemNets) :cite:`Sukhbaatar.Weston.Fergus.ea.2015` und den Neural Programmer-Interpreter :cite:`Reed.De-Freitas.2015` erlaubten statistischen Modellierern, iterative Ansätze zur Argumentation zu beschreiben. Diese Werkzeuge ermöglichen es, dass ein interner Zustand des tiefen Netzwerks wiederholt geändert wird, wodurch nachfolgende Schritte in einer Argumentationskette ausgeführt werden, ähnlich wie ein Prozessor Speicher für eine Berechnung ändern kann.
* Eine weitere wichtige Entwicklung war die Erfindung der GaNs :cite:`Goodfellow.Pouget-Abadie.Mirza.ea.2014`. Traditionell konzentrierten sich statistische Methoden zur Dichteschätzung und generative Modelle darauf, die richtigen Wahrscheinlichkeitsverteilungen und (oft ungefähre) Algorithmen für die Stichprobenentnahme von ihnen zu finden. Infolgedessen wurden diese Algorithmen durch die mangelnde Flexibilität, die den statistischen Modellen innewohnt, weitgehend eingeschränkt. Die entscheidende Innovation bei GaN bestand darin, den Sampler durch einen beliebigen Algorithmus mit differenzierbaren Parametern zu ersetzen. Diese werden dann so angepasst, dass der Diskriminator (im Grunde ein Test mit zwei Stichproben) Fälschungen nicht von realen Daten unterscheiden kann. Durch die Fähigkeit, beliebige Algorithmen zu verwenden, um Daten zu generieren, wurde die Dichteschätzung für eine Vielzahl von Techniken eröffnet. Beispiele für galoppierende Zebras :cite:`Zhu.Park.Isola.ea.2017` und gefälschte Berühmtheiten :cite:`Karras.Aila.Laine.ea.2017` sind beide ein Zeugnis für diesen Fortschritt. Selbst Amateur-Doodler können fotorealistische Bilder erstellen, die nur auf Skizzen basieren, die beschreiben, wie das Layout einer Szene aussieht wie :cite:`Park.Liu.Wang.ea.2019`.
* In vielen Fällen reicht eine einzelne GPU nicht aus, um die großen Datenmengen zu verarbeiten, die für Schulungen zur Verfügung stehen. In den letzten zehn Jahren hat sich die Fähigkeit, parallel verteilte Trainingsalgorithmen aufzubauen, deutlich verbessert. Eine der wichtigsten Herausforderungen bei der Entwicklung skalierbarer Algorithmen besteht darin, dass das Arbeitspferd der Deep Learning-Optimierung, stochastischer Gradientenabstieg, auf relativ kleine Minibatches von zu verarbeitenden Daten beruht. Gleichzeitig begrenzen kleine Chargen die Effizienz von GPUs. Daher entspricht das Training auf 1024 GPUs mit einer Minibatch-Größe von, sagen 32 Bildern pro Batch einer aggregierten Minibatch von 32.000 Bildern. Jüngste Arbeiten, zuerst von Li :cite:`Li.2017`, und anschließend von :cite:`You.Gitman.Ginsburg.2017` und :cite:`Jia.Song.He.ea.2018`, die Größe bis zu 64k Beobachtungen geschoben, wodurch die Trainingszeit für ResNet50 auf ImageNet auf weniger als 7 Minuten reduziert wurde. Zum Vergleichs—zunächst wurden die Trainingszeiten in der Reihenfolge der Tage gemessen.
* Die Fähigkeit, Berechnungen zu parallelisieren, hat auch ganz entscheidend zu Fortschritten im Verstärkungslernen beigetragen, zumindest wenn die Simulation eine Option ist. Dies hat zu erheblichen Fortschritten bei Computern geführt, die übermenschliche Leistung in Go, Atari-Spielen, Starcraft und in Physiksimulationen (z. B. mit MuJoCo) erreichten. Siehe z.B. :cite:`Silver.Huang.Maddison.ea.2016` für eine Beschreibung, wie dies in AlphaGo erreicht werden kann. Kurz gesagt: Verstärkungslernen funktioniert am besten, wenn viele (Staat, Aktion, Belohnung) Dreifacher verfügbar sind, d.h. wann immer es möglich ist, viele Dinge auszuprobieren, um zu erfahren, wie sie miteinander in Beziehung stehen. Simulation bietet eine solche Allee.
* Deep Learning-Frameworks haben eine entscheidende Rolle bei der Verbreitung von Ideen gespielt. Die erste Generation von Frameworks, die eine einfache Modellierung ermöglicht, umfasste [Caffe](https://github.com/BVLC/caffe), [Torch](https://github.com/torch) und [Theano](https://github.com/Theano/Theano). Viele bahnbrechende Papiere wurden mit diesen Werkzeugen geschrieben. Mittlerweile wurden sie durch [TensorFlow](https://github.com/tensorflow/tensorflow) ersetzt, die oft über seine High-Level-API [Keras](https://github.com/keras-team/keras), [CNTK](https://github.com/Microsoft/CNTK), [Caffe 2](https://github.com/caffe2/caffe2) und [Apache MxNet](https://github.com/apache/incubator-mxnet) verwendet werden. Die dritte Generation von Werkzeugen, nämlich Imperative Tools für Deep Learning, wurde vermutlich von [Chainer](https://github.com/chainer/chainer) angeführt, die eine Syntax verwendet, die Python NumPy ähnlich ist, um Modelle zu beschreiben. Diese Idee wurde sowohl von [PyTorch](https://github.com/pytorch/pytorch), [Gluon API](https://github.com/apache/incubator-mxnet) von MXNet als auch von [Jax](https://github.com/google/jax) übernommen. Es ist die letztere Gruppe, die dieser Kurs verwendet, um Deep Learning zu lehren.

Die Arbeitsteilung zwischen Systemforschern, die bessere Werkzeuge bauen, und statistische Modellierer, die bessere Netzwerke aufbauen, hat die Dinge erheblich vereinfacht. Zum Beispiel war die Ausbildung eines linearen logistischen Regressionsmodells ein nicht-triviales Hausaufgabenproblem, das es wert ist, neue Doktoranden des maschinellen Lernens an der Carnegie Mellon University im Jahr 2014 zu geben. Mittlerweile kann diese Aufgabe mit weniger als 10 Zeilen Code erledigt werden, wodurch sie den Programmierern fest in den Griff gebracht wird.

## Erfolgsgeschichten

Künstliche Intelligenz hat eine lange Geschichte darin, Ergebnisse zu liefern, die sonst schwer zu erreichen wären. Zum Beispiel wird Mail mit optischer Zeichenerkennung sortiert. Diese Systeme wurden seit den 90er Jahren eingesetzt (dies ist schließlich die Quelle der berühmten MNIST und USPS Sätze handschriftlicher Ziffern). Gleiches gilt für das Lesen von Schecks für Bankeinlagen und die Bewertung der Bonität der Antragsteller. Finanztransaktionen werden automatisch auf Betrug überprüft. Dies bildet das Rückgrat vieler E-Commerce-Zahlungssysteme, wie PayPal, Stripe, AliPay, WeChat, Apple, Visa, MasterCard. Computerprogramme für Schach sind seit Jahrzehnten konkurrenzfähig. Machine Learning Feeds Suche, Empfehlung, Personalisierung und Ranking im Internet. Mit anderen Worten, künstliche Intelligenz und maschinelles Lernen sind weit verbreitet, wenn auch oft vor den Augen verborgen.

Es ist erst kürzlich, dass KI im Rampenlicht stand, vor allem aufgrund von Lösungen für Probleme, die zuvor als hartnäckig angesehen wurden.

* Intelligente Assistenten wie Apples Siri, Amazons Alexa oder Google-Assistent sind in der Lage, gesprochene Fragen mit einem angemessenen Maß an Genauigkeit zu beantworten. Dazu gehören schwerwiegende Aufgaben wie das Einschalten von Lichtschaltern (ein Segen für Behinderte) bis hin zu Friseurterminen und Telefon-Support-Dialog. Dies ist wahrscheinlich das auffälligste Zeichen dafür, dass KI unser Leben beeinflusst.
* Ein wichtiger Bestandteil der digitalen Assistenten ist die Fähigkeit, Sprache genau zu erkennen. Allmählich hat sich die Genauigkeit solcher Systeme bis zu dem Punkt erhöht, an dem sie für bestimmte Anwendungen die menschliche Parität :cite:`Xiong.Wu.Alleva.ea.2018` erreichen.
* Auch die Objekterkennung ist ein langer Weg gekommen. Die Einschätzung des Objekts in einem Bild war 2010 eine ziemlich herausfordernde Aufgabe. Auf dem ImageNet-Benchmark erreichte :cite:`Lin.Lv.Zhu.ea.2010` eine Top-5-Fehlerrate von 28%. Bis 2017 reduzierte :cite:`Hu.Shen.Sun.2018` diese Fehlerrate auf 2,25%. In ähnlicher Weise wurden erstaunliche Ergebnisse erzielt, um Vögel zu identifizieren oder Hautkrebs zu diagnostizieren.
* Spiele wurden früher eine Bastion der menschlichen Intelligenz. Ausgehend von TDGammon [23], ein Programm zum Abspielen von Backgammon mit zeitlicher Differenz (TD) Verstärkung Lernen, algorithmischen und rechnerischen Fortschritt hat zu Algorithmen für eine breite Palette von Anwendungen geführt. Im Gegensatz zu Backgammon hat Schach einen viel komplexeren Zustand Raum und eine Reihe von Aktionen. DeepBlue schlug Garry Kasparov, Campbell et al. :cite:`Campbell.Hoane-Jr.Hsu.2002`, mit massiver Parallelität, Spezialhardware und effizienter Suche durch den Spielbaum. Go ist noch schwieriger, aufgrund seines riesigen Zustandsraums. AlphaGo erreichte die menschliche Parität im Jahr 2015, :cite:`Silver.Huang.Maddison.ea.2016` mit Deep Learning kombiniert mit Monte Carlo Baum-Sampling. Die Herausforderung bei Poker war, dass der Staatsraum groß ist und nicht vollständig beobachtet wird (wir kennen die Karten des Gegners nicht). Libratus übertraf die menschliche Leistung in Poker mit effizient strukturierten Strategien :cite:`Brown.Sandholm.2017`. Dies zeigt den beeindruckenden Fortschritt in Spielen und die Tatsache, dass fortschrittliche Algorithmen eine entscheidende Rolle dabei gespielt haben.
* Ein weiterer Hinweis auf Fortschritte in der KI ist das Aufkommen von selbstfahrenden Autos und LKWs. Obwohl die vollständige Autonomie noch nicht ganz in Reichweite ist, wurden in dieser Richtung hervorragende Fortschritte erzielt, wobei Unternehmen wie Tesla, NVIDIA und Waymo Produkte versenden, die zumindest teilweise Autonomie ermöglichen. Was die volle Autonomie so herausfordernd macht, ist, dass das richtige Fahren die Fähigkeit erfordert, Regeln in ein System wahrzunehmen, zu begründen und zu integrieren. Gegenwärtig wird Deep Learning in erster Linie im Computer-Vision-Aspekt dieser Probleme verwendet. Der Rest wird stark von Ingenieuren abgestimmt.

Auch hier kratzt die obige Liste kaum an der Oberfläche, wo maschinelles Lernen praktische Anwendungen beeinflusst hat. Beispielsweise verdanken Robotik, Logistik, Computerbiologie, Teilchenphysik und Astronomie einige ihrer beeindruckendsten jüngsten Fortschritte zumindest teilweise dem maschinellen Lernen. ML wird damit zu einem allgegenwärtigen Werkzeug für Ingenieure und Wissenschaftler.

Häufig wurde die Frage nach der AI-Apokalypse oder der KI-Singularität in nicht-technischen Artikeln über KI aufgeworfen. Die Angst ist, dass maschinelles Lernen Systeme fühlend werden und unabhängig von ihren Programmierern (und Meistern) über Dinge entscheiden, die den Lebensunterhalt des Menschen direkt beeinflussen. Bis zu einem gewissen Grad beeinflusst KI bereits sofort den Lebensunterhalt von Menschen — die Kreditwürdigkeit wird automatisch beurteilt, Autopiloten navigieren meist Fahrzeuge, Entscheidungen über die Gewährung von Kaution verwenden statistische Daten als Eingabe. Frivoler können wir Alexa bitten, die Kaffeemaschine einzuschalten.

Glücklicherweise sind wir weit entfernt von einem fühlenden KI-System, das bereit ist, seine menschlichen Schöpfer zu manipulieren (oder ihren Kaffee zu verbrennen). Erstens werden KI-Systeme auf spezifische, zielorientierte Weise entwickelt, geschult und eingesetzt. Obwohl ihr Verhalten die Illusion allgemeiner Intelligenz vermitteln könnte, ist es eine Kombination von Regeln, Heuristiken und statistischen Modellen, die dem Design zugrunde liegen. Zweitens gibt es derzeit Werkzeuge für *künstliche allgemeine Intelligenz* einfach nicht, die sich selbst verbessern können, Vernunft über sich selbst, und die in der Lage sind, ihre eigene Architektur zu modifizieren, zu erweitern und zu verbessern, während sie versuchen, allgemeine Aufgaben zu lösen.

Eine viel dringlichere Sorge ist, wie KI in unserem täglichen Leben eingesetzt wird. Es ist wahrscheinlich, dass viele wenige Aufgaben, die von Lkw-Fahrern und Ladenassistenten erfüllt werden, automatisiert werden können und werden. Landwirtschaftsroboter werden wahrscheinlich die Kosten für den ökologischen Landbau senken, aber sie werden auch die Erntevorgänge automatisieren. Diese Phase der industriellen Revolution kann tiefgreifende Folgen für große Teile der Gesellschaft haben (Lkw-Fahrer und Ladenassistenten gehören zu den häufigsten Arbeitsplätzen in vielen Staaten). Darüber hinaus können statistische Modelle, wenn sie ohne Sorgfalt angewendet werden, zu Rassen-, Geschlechts- oder Altersverzerrungen führen und angemessene Bedenken hinsichtlich der Verfahrensgerechtigkeit aufwerfen, wenn sie automatisiert sind, um Folgeentscheidungen voranzutreiben. Es ist wichtig sicherzustellen, dass diese Algorithmen mit Sorgfalt verwendet werden. Mit dem, was wir heute wissen, wird uns dies eine viel dringendere Sorge als das Potenzial bösartiger Superintelligenz, die Menschheit zu zerstören.

## Zusammenfassung

* Machine Learning untersucht, wie Computersysteme *Erfahrung* (häufig Daten) nutzen können, um die Leistung bei bestimmten Aufgaben zu verbessern. Es kombiniert Ideen aus Statistiken, Data Mining, künstlicher Intelligenz und Optimierung. Oft wird es als Mittel zur Implementierung von künstlich intelligenten Lösungen verwendet.
* Als Klasse des maschinellen Lernens konzentriert sich das repräsentative Lernen darauf, wie automatisch die geeignete Methode zur Darstellung von Daten gefunden werden kann. Dies wird oft durch eine Progression gelernter Transformationen erreicht.
* Ein Großteil der jüngsten Fortschritte im Deep Learning wurde durch eine Fülle von Daten ausgelöst, die sich aus billigen Sensoren und Anwendungen im Internet ergeben, und durch erhebliche Fortschritte bei der Berechnung, hauptsächlich durch GPUs.
* Die Optimierung des gesamten Systems ist eine Schlüsselkomponente bei der Erzielung einer guten Leistung. Die Verfügbarkeit effizienter Deep Learning-Frameworks hat das Design und die Implementierung erheblich vereinfacht.

## Übungen

1. Welche Teile des Codes, die Sie gerade schreiben, könnten „gelernt“ werden, dh durch Lernen verbessert und automatisch Designentscheidungen bestimmt werden, die in Ihrem Code getroffen werden? Enthält Ihr Code heuristische Entwurfsoptionen?
1. Welche Probleme, denen Sie begegnen, haben viele Beispiele, wie Sie sie lösen können, aber keine spezifische Möglichkeit, sie zu automatisieren? Dies können Hauptkandidaten für die Verwendung von Deep Learning sein.
1. Was ist die Beziehung zwischen Algorithmen und Daten, die die Entwicklung künstlicher Intelligenz als eine neue industrielle Revolution betrachten? Ist es ähnlich wie Dampfmaschinen und Kohle (was ist der grundlegende Unterschied)?
1. Wo sonst können Sie den End-to-End-Schulungsansatz anwenden? Physik? Ingenieurwesen? Ökonometrie?

[Discussions](https://discuss.d2l.ai/t/22)
